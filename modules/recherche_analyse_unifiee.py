# modules/recherche_analyse_unifiee.py
"""Module unifiÃ© de recherche universelle et analyse IA avec comprÃ©hension du langage naturel"""

import asyncio
import html
import re
import sys
import time
from datetime import datetime
from difflib import SequenceMatcher
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
import streamlit as st
from streamlit_shortcuts import add_keyboard_shortcuts

# Ajouter le chemin parent pour importer utils
sys.path.append(str(Path(__file__).parent.parent))
try:
    from utils import clean_key, format_legal_date, truncate_text
except ImportError:
    # Les utilitaires ne sont pas disponibles
    pass
from utils import clean_key, format_legal_date, truncate_text
from utils.decorators import decorate_public_functions

# Enregistrement automatique des fonctions publiques pour le module
decorate_public_functions(sys.modules[__name__])

# ========================= LAZY LOADING & IMPORTS =========================

# Variables globales pour le lazy loading
_imports_loaded = False
_modules_cache = {}

def lazy_load_imports():
    """Charge les imports de maniÃ¨re lazy pour amÃ©liorer les performances"""
    global _imports_loaded
    if _imports_loaded:
        return
    
    # Progress bar pour le chargement
    progress_bar = st.progress(0)
    status = st.empty()
    
    try:
        # Configuration et dataclasses
        status.text("â³ Chargement des configurations...")
        progress_bar.progress(10)
        
        from config.app_config import (ANALYSIS_PROMPTS_AFFAIRES,
                                       InfractionAffaires, LLMProvider)
        _modules_cache['config'] = {
            'InfractionAffaires': InfractionAffaires,
            'ANALYSIS_PROMPTS_AFFAIRES': ANALYSIS_PROMPTS_AFFAIRES,
            'LLMProvider': LLMProvider
        }
        
        # Services de recherche
        status.text("â³ Chargement des services de recherche...")
        progress_bar.progress(25)
        
        try:
            from managers.universal_search_service import (
                UniversalSearchService, get_universal_search_service)
            _modules_cache['search_service'] = {
                'UniversalSearchService': UniversalSearchService,
                'get_universal_search_service': get_universal_search_service,
                'available': True
            }
        except ImportError:
            _modules_cache['search_service'] = {'available': False}
        
        # Managers IA
        status.text("â³ Chargement des managers IA...")
        progress_bar.progress(40)
        
        try:
            from managers.multi_llm_manager import MultiLLMManager
            _modules_cache['multi_llm'] = {'MultiLLMManager': MultiLLMManager, 'available': True}
        except ImportError:
            _modules_cache['multi_llm'] = {'available': False}
        
        try:
            from managers.jurisprudence_verifier import (
                JurisprudenceVerifier, display_jurisprudence_verification)
            _modules_cache['jurisprudence'] = {
                'JurisprudenceVerifier': JurisprudenceVerifier,
                'display_jurisprudence_verification': display_jurisprudence_verification,
                'available': True
            }
        except ImportError:
            _modules_cache['jurisprudence'] = {'available': False}
        
        # APIs et utils
        status.text("â³ Chargement des APIs...")
        progress_bar.progress(60)
        
        try:
            from utils.api_utils import call_llm_api, get_available_models
            _modules_cache['api_utils'] = {
                'get_available_models': get_available_models,
                'call_llm_api': call_llm_api,
                'available': True
            }
        except ImportError:
            _modules_cache['api_utils'] = {'available': False}
        
        # Modules juridiques
        status.text("â³ Chargement des modules juridiques...")
        progress_bar.progress(80)
        
        try:
            from modules.generation_longue import (
                GenerateurDocumentsLongs, show_generation_longue_interface)
            _modules_cache['generation_longue'] = {
                'GenerateurDocumentsLongs': GenerateurDocumentsLongs,
                'show_generation_longue_interface': show_generation_longue_interface,
                'available': True
            }
        except ImportError:
            _modules_cache['generation_longue'] = {'available': False}
        
        # Finalisation
        progress_bar.progress(100)
        status.text("âœ… Modules chargÃ©s avec succÃ¨s!")
        time.sleep(0.5)
        
        _imports_loaded = True
        
    except Exception as e:
        st.error(f"âŒ Erreur lors du chargement des modules : {str(e)}")
    finally:
        progress_bar.empty()
        status.empty()

def get_module(module_name, item_name=None):
    """RÃ©cupÃ¨re un module ou un Ã©lÃ©ment de module de maniÃ¨re lazy"""
    if not _imports_loaded:
        lazy_load_imports()
    
    module = _modules_cache.get(module_name, {})
    if item_name:
        return module.get(item_name)
    return module

# ========================= FONCTION PRINCIPALE RUN =========================

def run():
    """Fonction principale du module - Point d'entrÃ©e pour le lazy loading"""
    
    # Chargement lazy des imports
    if not _imports_loaded:
        with st.spinner("ğŸš€ Initialisation du module de recherche avancÃ©e..."):
            lazy_load_imports()
    
    # CSS personnalisÃ© pour amÃ©liorer le design
    st.markdown("""
    <style>
    /* Animation de fade-in pour les Ã©lÃ©ments */
    @keyframes fadeIn {
        from { opacity: 0; transform: translateY(10px); }
        to { opacity: 1; transform: translateY(0); }
    }
    
    .stTabs [data-baseweb="tab-list"] {
        gap: 24px;
    }
    
    .stTabs [data-baseweb="tab"] {
        padding: 12px 24px;
        font-weight: 500;
        animation: fadeIn 0.3s ease-out;
    }
    
    /* Cards effect */
    .result-card {
        background: #f8f9fa;
        border: 1px solid #e0e0e0;
        border-radius: 12px;
        padding: 20px;
        margin: 10px 0;
        transition: all 0.3s ease;
        animation: fadeIn 0.5s ease-out;
    }
    
    .result-card:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        border-color: #4285f4;
    }
    
    /* Progress indicators */
    .progress-step {
        display: flex;
        align-items: center;
        padding: 12px;
        margin: 8px 0;
        background: #f0f7ff;
        border-radius: 8px;
        animation: fadeIn 0.3s ease-out;
    }
    
    .progress-step.active {
        background: #e3f2fd;
        border-left: 4px solid #2196f3;
    }
    
    .progress-step.completed {
        background: #e8f5e9;
        border-left: 4px solid #4caf50;
    }
    
    /* Search bar enhancement */
    .search-container {
        position: relative;
        animation: fadeIn 0.5s ease-out;
    }
    
    /* AI model selector */
    .ai-model-card {
        padding: 16px;
        border: 2px solid #e0e0e0;
        border-radius: 8px;
        margin: 8px 0;
        cursor: pointer;
        transition: all 0.2s ease;
    }
    
    .ai-model-card:hover {
        border-color: #4285f4;
        background: #f8f9fa;
    }
    
    .ai-model-card.selected {
        border-color: #4285f4;
        background: #e3f2fd;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # Header avec titre et description
    st.title("ğŸ” Recherche & Analyse UnifiÃ©e avec IA")
    st.markdown("""
    <div style="animation: fadeIn 0.6s ease-out;">
        <p style="font-size: 1.1em; color: #666;">
        Exploitez la puissance de l'IA pour rechercher, analyser et gÃ©nÃ©rer du contenu juridique.
        Ã‰crivez naturellement ce que vous souhaitez faire.
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # Initialiser l'interface si nÃ©cessaire
    if 'unified_interface' not in st.session_state:
        st.session_state.unified_interface = UnifiedSearchAnalysisInterface()
    
    # Options de configuration rapide
    with st.expander("âš™ï¸ Configuration rapide", expanded=False):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.session_state.show_nl_analysis = st.checkbox(
                "ğŸ§  Afficher l'analyse IA",
                value=st.session_state.get('show_nl_analysis', False),
                help="Voir comment l'IA comprend votre requÃªte"
            )
        
        with col2:
            st.session_state.auto_enhance = st.checkbox(
                "âœ¨ AmÃ©lioration automatique",
                value=st.session_state.get('auto_enhance', True),
                help="L'IA amÃ©liore automatiquement vos requÃªtes"
            )
        
        with col3:
            st.session_state.multi_ai_mode = st.checkbox(
                "ğŸ¤– Mode Multi-IA",
                value=st.session_state.get('multi_ai_mode', False),
                help="Utiliser plusieurs IA simultanÃ©ment"
            )
    
    # Actions rapides avec design amÃ©liorÃ©
    st.markdown("### ğŸš€ Actions rapides")
    show_enhanced_quick_actions()
    
    # Interface principale en onglets
    tabs = st.tabs([
        "ğŸ” Recherche & Analyse",
        "ğŸ“ GÃ©nÃ©ration",
        "ğŸ¤– Multi-IA",
        "ğŸ“Š RÃ©sultats",
        "ğŸ”§ AvancÃ©"
    ])
    
    with tabs[0]:  # Recherche & Analyse
        show_search_analysis_tab()
    
    with tabs[1]:  # GÃ©nÃ©ration
        show_generation_tab()
    
    with tabs[2]:  # Multi-IA
        show_multi_ai_tab()
    
    with tabs[3]:  # RÃ©sultats
        show_results_tab()
    
    with tabs[4]:  # AvancÃ©
        show_advanced_tab()
    
    # Footer avec informations
    show_enhanced_footer()

# ========================= INTERFACES PAR ONGLETS =========================

def show_search_analysis_tab():
    """Onglet de recherche et analyse"""
    
    # Interface de recherche principale
    col1, col2 = st.columns([5, 1])
    
    with col1:
        # Formulaire de recherche avec design amÃ©liorÃ©
        with st.form(key='main_search_form', clear_on_submit=False):
            query = st.text_area(
                "ğŸ” Votre demande",
                value=st.session_state.get('universal_query', ''),
                placeholder="""Exemples :
â€¢ Analyser les risques dans l'affaire Martin
â€¢ Rechercher tous les contrats de 2024
â€¢ PrÃ©parer mon client pour l'audience de demain
â€¢ RÃ©diger une plainte exhaustive contre Vinci""",
                height=120,
                key="search_query_input"
            )
            
            col_form1, col_form2, col_form3 = st.columns([2, 1, 1])
            
            with col_form1:
                search_type = st.selectbox(
                    "Type de recherche",
                    ["ğŸ” Universelle", "ğŸ“š Juridique", "ğŸ“„ Documents", "ğŸ¯ SpÃ©cifique"],
                    key="search_type_select"
                )
            
            with col_form2:
                submit = st.form_submit_button(
                    "ğŸš€ Lancer",
                    type="primary",
                    use_container_width=True
                )
            
            with col_form3:
                clear = st.form_submit_button(
                    "ğŸ”„ Effacer",
                    use_container_width=True
                )
    
    with col2:
        # Suggestions contextuelles
        st.markdown("### ğŸ’¡ Suggestions")
        if st.button("ğŸ“‹ Plainte", use_container_width=True):
            st.session_state.pending_query = "RÃ©diger une plainte contre"
            st.rerun()
        
        if st.button("ğŸ“Š Analyse", use_container_width=True):
            st.session_state.pending_query = "Analyser les risques"
            st.rerun()
        
        if st.button("ğŸ“ SynthÃ¨se", use_container_width=True):
            st.session_state.pending_query = "Faire une synthÃ¨se"
            st.rerun()
    
    # Traitement de la requÃªte
    if submit and query:
        with st.spinner("ğŸ”„ Traitement de votre demande..."):
            # Animation de progression
            progress_container = st.container()
            with progress_container:
                show_processing_animation(query)
            
            # Traitement asynchrone
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                interface = st.session_state.unified_interface
                loop.run_until_complete(interface.process_universal_query(query))
            finally:
                loop.close()
    
    # Affichage des rÃ©sultats en temps rÃ©el
    if st.session_state.get('search_results') or st.session_state.get('analysis_results'):
        show_enhanced_results()

def show_generation_tab():
    """Onglet de gÃ©nÃ©ration de documents"""
    
    st.markdown("### ğŸ“ GÃ©nÃ©ration de documents juridiques")
    
    # SÃ©lection du type de document
    col1, col2 = st.columns([2, 1])
    
    with col1:
        doc_type = st.selectbox(
            "Type de document",
            [
                "ğŸ“‹ Plainte avec constitution de partie civile",
                "ğŸ“‘ Conclusions",
                "ğŸ“„ MÃ©moire",
                "âœ‰ï¸ Courrier",
                "ğŸ“œ Citation directe",
                "ğŸ”– RequÃªte",
                "ğŸ“Š Rapport d'expertise"
            ],
            key="generation_doc_type"
        )
    
    with col2:
        complexity = st.radio(
            "ComplexitÃ©",
            ["âš¡ Simple", "ğŸ“„ Standard", "ğŸ“š Exhaustif"],
            key="generation_complexity"
        )
    
    # Configuration de la gÃ©nÃ©ration
    with st.expander("âš™ï¸ Configuration avancÃ©e", expanded=True):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("**ğŸ“Š Parties**")
            demandeurs = st.text_area(
                "Demandeurs",
                placeholder="Un par ligne",
                height=100,
                key="gen_demandeurs"
            )
            
            defendeurs = st.text_area(
                "DÃ©fendeurs",
                placeholder="Un par ligne",
                height=100,
                key="gen_defendeurs"
            )
        
        with col2:
            st.markdown("**ğŸ¯ Ã‰lÃ©ments juridiques**")
            infractions = st.text_area(
                "Infractions",
                placeholder="Une par ligne",
                height=100,
                key="gen_infractions"
            )
            
            reference = st.text_input(
                "RÃ©fÃ©rence dossier",
                placeholder="Ex: MARTIN_2024",
                key="gen_reference"
            )
        
        with col3:
            st.markdown("**ğŸ¤– ModÃ¨les IA**")
            
            # SÃ©lection des modÃ¨les avec design amÃ©liorÃ©
            available_models = get_available_ai_models()
            selected_models = []
            
            for model in available_models:
                col_model = st.container()
                with col_model:
                    if st.checkbox(
                        f"{model['icon']} {model['name']}",
                        value=model['default'],
                        key=f"model_{model['id']}"
                    ):
                        selected_models.append(model['id'])
            
            fusion_mode = st.radio(
                "Mode de fusion",
                ["ğŸ§¬ SynthÃ¨se IA", "ğŸ“‘ ConcatÃ©nation", "ğŸ† Meilleur rÃ©sultat"],
                key="gen_fusion_mode"
            )
    
    # Options supplÃ©mentaires
    col1, col2, col3 = st.columns(3)
    
    with col1:
        include_jurisprudence = st.checkbox(
            "ğŸ“š Inclure jurisprudences",
            value=True,
            key="gen_include_juris"
        )
    
    with col2:
        include_chronologie = st.checkbox(
            "ğŸ“… Chronologie dÃ©taillÃ©e",
            value=True,
            key="gen_include_chrono"
        )
    
    with col3:
        enrich_parties = st.checkbox(
            "ğŸ¢ Enrichir les sociÃ©tÃ©s",
            value=True,
            key="gen_enrich_parties"
        )
    
    # Bouton de gÃ©nÃ©ration avec style
    if st.button(
        "ğŸš€ GÃ©nÃ©rer le document",
        type="primary",
        use_container_width=True,
        key="generate_document_btn"
    ):
        if selected_models:
            generate_with_multi_ai(
                doc_type=doc_type,
                complexity=complexity,
                models=selected_models,
                config={
                    'demandeurs': demandeurs.split('\n') if demandeurs else [],
                    'defendeurs': defendeurs.split('\n') if defendeurs else [],
                    'infractions': infractions.split('\n') if infractions else [],
                    'reference': reference,
                    'include_jurisprudence': include_jurisprudence,
                    'include_chronologie': include_chronologie,
                    'enrich_parties': enrich_parties,
                    'fusion_mode': fusion_mode
                }
            )
        else:
            st.error("âŒ Veuillez sÃ©lectionner au moins un modÃ¨le IA")

def show_multi_ai_tab():
    """Onglet de comparaison multi-IA"""
    
    st.markdown("### ğŸ¤– Comparaison Multi-IA")
    
    # Interface de prompt
    prompt = st.text_area(
        "ğŸ“ Votre prompt",
        placeholder="Entrez votre demande pour comparer les rÃ©ponses de diffÃ©rentes IA",
        height=150,
        key="multi_ai_prompt"
    )
    
    # SÃ©lection des modÃ¨les avec cards visuelles
    st.markdown("#### ğŸ¤– SÃ©lectionnez les modÃ¨les Ã  comparer")
    
    models = get_available_ai_models()
    selected_models = []
    
    cols = st.columns(3)
    for idx, model in enumerate(models):
        with cols[idx % 3]:
            # Card pour chaque modÃ¨le
            with st.container():
                st.markdown(f"""
                <div class="ai-model-card" id="model_{model['id']}">
                    <h4>{model['icon']} {model['name']}</h4>
                    <p style="color: #666; font-size: 0.9em;">{model['description']}</p>
                    <p style="color: #888; font-size: 0.8em;">
                        Vitesse: {model['speed']} | QualitÃ©: {model['quality']}
                    </p>
                </div>
                """, unsafe_allow_html=True)
                
                if st.checkbox(
                    "Utiliser",
                    key=f"multi_select_{model['id']}",
                    label_visibility="collapsed"
                ):
                    selected_models.append(model)
    
    # Options de comparaison
    with st.expander("âš™ï¸ Options de comparaison", expanded=True):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            comparison_mode = st.selectbox(
                "Mode de comparaison",
                ["âš¡ ParallÃ¨le", "ğŸ”„ SÃ©quentiel", "ğŸ† Tournoi", "ğŸ§¬ Consensus"],
                help="""
                â€¢ ParallÃ¨le : Tous en mÃªme temps
                â€¢ SÃ©quentiel : Un par un avec amÃ©lioration
                â€¢ Tournoi : Ã‰limination progressive
                â€¢ Consensus : Trouve le meilleur compromis
                """
            )
        
        with col2:
            metrics = st.multiselect(
                "MÃ©triques d'Ã©valuation",
                ["ğŸ“Š QualitÃ©", "ğŸ¯ Pertinence", "ğŸ’¡ CrÃ©ativitÃ©", 
                 "ğŸ“ CohÃ©rence", "âš¡ RapiditÃ©", "ğŸ“š ExhaustivitÃ©"],
                default=["ğŸ“Š QualitÃ©", "ğŸ¯ Pertinence"]
            )
        
        with col3:
            temperature = st.slider(
                "ğŸŒ¡ï¸ TempÃ©rature",
                0.0, 1.0, 0.3, 0.1,
                help="Plus bas = Plus factuel | Plus haut = Plus crÃ©atif"
            )
    
    # Lancement de la comparaison
    if st.button(
        "ğŸš€ Lancer la comparaison",
        type="primary",
        use_container_width=True,
        disabled=not (prompt and selected_models)
    ):
        run_multi_ai_comparison(
            prompt=prompt,
            models=selected_models,
            mode=comparison_mode,
            metrics=metrics,
            temperature=temperature
        )

def show_results_tab():
    """Onglet des rÃ©sultats"""
    
    # VÃ©rifier s'il y a des rÃ©sultats
    has_results = any([
        st.session_state.get('search_results'),
        st.session_state.get('analysis_results'),
        st.session_state.get('generated_content'),
        st.session_state.get('multi_ai_results')
    ])
    
    if not has_results:
        st.info("ğŸ“Š Aucun rÃ©sultat disponible. Lancez d'abord une recherche, analyse ou gÃ©nÃ©ration.")
        return
    
    # Navigation entre les diffÃ©rents types de rÃ©sultats
    result_types = []
    if st.session_state.get('search_results'):
        result_types.append("ğŸ” Recherche")
    if st.session_state.get('analysis_results'):
        result_types.append("ğŸ“Š Analyse")
    if st.session_state.get('generated_content'):
        result_types.append("ğŸ“ GÃ©nÃ©ration")
    if st.session_state.get('multi_ai_results'):
        result_types.append("ğŸ¤– Multi-IA")
    
    selected_result = st.radio(
        "Type de rÃ©sultat",
        result_types,
        horizontal=True,
        key="result_type_selector"
    )
    
    # Affichage selon le type sÃ©lectionnÃ©
    if "ğŸ” Recherche" in selected_result:
        show_search_results_enhanced()
    elif "ğŸ“Š Analyse" in selected_result:
        show_analysis_results_enhanced()
    elif "ğŸ“ GÃ©nÃ©ration" in selected_result:
        show_generation_results_enhanced()
    elif "ğŸ¤– Multi-IA" in selected_result:
        show_multi_ai_results_enhanced()
    
    # Actions globales sur les rÃ©sultats
    st.markdown("---")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if st.button("ğŸ’¾ Tout sauvegarder", use_container_width=True):
            save_all_results()
    
    with col2:
        if st.button("ğŸ“¤ Exporter", use_container_width=True):
            export_results_dialog()
    
    with col3:
        if st.button("ğŸ”— Partager", use_container_width=True):
            share_results_dialog()
    
    with col4:
        if st.button("ğŸ—‘ï¸ Effacer tout", use_container_width=True):
            if st.button("âš ï¸ Confirmer", key="confirm_clear"):
                clear_all_results()
                st.rerun()

def show_advanced_tab():
    """Onglet des fonctionnalitÃ©s avancÃ©es"""
    
    st.markdown("### ğŸ”§ FonctionnalitÃ©s avancÃ©es")
    
    # Sous-onglets pour les diffÃ©rentes fonctionnalitÃ©s
    sub_tabs = st.tabs([
        "ğŸ“Š Statistiques",
        "ğŸ” Recherche avancÃ©e",
        "ğŸ§¬ Fusion IA",
        "ğŸ“ˆ Analytics",
        "âš™ï¸ Configuration"
    ])
    
    with sub_tabs[0]:  # Statistiques
        show_work_statistics_enhanced()
    
    with sub_tabs[1]:  # Recherche avancÃ©e
        show_advanced_search_interface()
    
    with sub_tabs[2]:  # Fusion IA
        show_ai_fusion_interface()
    
    with sub_tabs[3]:  # Analytics
        show_analytics_dashboard()
    
    with sub_tabs[4]:  # Configuration
        show_configuration_interface()

# ========================= FONCTIONS D'INTERFACE AMÃ‰LIORÃ‰ES =========================

def show_enhanced_quick_actions():
    """Actions rapides avec design amÃ©liorÃ©"""
    
    # Container avec animation
    with st.container():
        st.markdown("""
        <div style="animation: fadeIn 0.8s ease-out;">
        """, unsafe_allow_html=True)
        
        cols = st.columns(6)
        
        actions = [
            ("ğŸ“ RÃ©daction", "CrÃ©er un document", "primary"),
            ("ğŸ¤– Analyse IA", "Analyser avec l'IA", "secondary"),
            ("ğŸ” Recherche", "Rechercher", "secondary"),
            ("ğŸ“Š SynthÃ¨se", "SynthÃ©tiser", "secondary"),
            ("ğŸ“‹ Bordereau", "CrÃ©er bordereau", "secondary"),
            ("âš–ï¸ Juridique", "Actes juridiques", "primary")
        ]
        
        for idx, (icon_text, help_text, button_type) in enumerate(actions):
            with cols[idx]:
                if button_type == "primary":
                    if st.button(
                        icon_text,
                        help=help_text,
                        use_container_width=True,
                        type="primary",
                        key=f"quick_action_{idx}"
                    ):
                        handle_quick_action(icon_text)
                else:
                    if st.button(
                        icon_text,
                        help=help_text,
                        use_container_width=True,
                        key=f"quick_action_{idx}"
                    ):
                        handle_quick_action(icon_text)
        
        st.markdown("</div>", unsafe_allow_html=True)

def show_processing_animation(query: str):
    """Animation de traitement avec Ã©tapes visuelles"""
    
    steps = [
        ("ğŸ§  Analyse de votre demande", 20),
        ("ğŸ” Recherche dans la base", 40),
        ("ğŸ¤– Traitement par l'IA", 60),
        ("ğŸ“Š PrÃ©paration des rÃ©sultats", 80),
        ("âœ… Finalisation", 100)
    ]
    
    progress = st.progress(0)
    status_container = st.empty()
    
    for step_text, progress_value in steps:
        status_container.markdown(f"""
        <div class="progress-step {'active' if progress_value < 100 else 'completed'}">
            <span style="font-size: 1.2em; margin-right: 10px;">
                {'â³' if progress_value < 100 else 'âœ…'}
            </span>
            <span>{step_text}</span>
        </div>
        """, unsafe_allow_html=True)
        
        progress.progress(progress_value / 100)
        time.sleep(0.5)
    
    # Nettoyer aprÃ¨s l'animation
    progress.empty()
    status_container.empty()

def show_enhanced_results():
    """Affichage amÃ©liorÃ© des rÃ©sultats avec animations"""
    
    results = st.session_state.get('search_results', [])
    
    if not results:
        return
    
    st.markdown("### ğŸ“Š RÃ©sultats")
    
    # MÃ©triques de rÃ©sultats
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "Documents trouvÃ©s",
            len(results),
            delta=f"+{len(results)}" if len(results) > 0 else None
        )
    
    with col2:
        avg_score = sum(r.get('score', 0) for r in results) / len(results) if results else 0
        st.metric(
            "Pertinence moyenne",
            f"{avg_score:.0%}"
        )
    
    with col3:
        categories = set(r.get('category', 'Autre') for r in results)
        st.metric(
            "CatÃ©gories",
            len(categories)
        )
    
    with col4:
        recent = sum(1 for r in results if is_recent_document(r))
        st.metric(
            "Documents rÃ©cents",
            recent
        )
    
    # Filtres dynamiques
    with st.expander("ğŸ” Filtres", expanded=True):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            category_filter = st.multiselect(
                "CatÃ©gories",
                list(categories),
                key="filter_categories"
            )
        
        with col2:
            score_filter = st.slider(
                "Pertinence minimale",
                0, 100, 0,
                format="%d%%",
                key="filter_score"
            )
        
        with col3:
            sort_by = st.selectbox(
                "Trier par",
                ["Pertinence", "Date", "Titre", "CatÃ©gorie"],
                key="sort_results_by"
            )
    
    # Affichage des rÃ©sultats avec cards
    filtered_results = filter_results(results, category_filter, score_filter)
    sorted_results = sort_results(filtered_results, sort_by)
    
    for idx, result in enumerate(sorted_results[:20]):
        display_result_card(result, idx)

def display_result_card(result: dict, index: int):
    """Affiche un rÃ©sultat sous forme de card interactive"""
    
    with st.container():
        st.markdown(f"""
        <div class="result-card" style="animation-delay: {index * 0.1}s;">
        """, unsafe_allow_html=True)
        
        col1, col2, col3 = st.columns([4, 1, 1])
        
        with col1:
            st.markdown(f"**{result.get('title', 'Sans titre')}**")
            
            # Tags et mÃ©tadonnÃ©es
            tags_html = ""
            if result.get('category'):
                tags_html += f'<span class="tag">{result["category"]}</span> '
            if result.get('date'):
                tags_html += f'<span class="tag">ğŸ“… {format_legal_date(result["date"])}</span> '
            if result.get('score'):
                score_class = "high" if result['score'] > 0.8 else "medium" if result['score'] > 0.5 else "low"
                tags_html += f'<span class="tag score-{score_class}">ğŸ¯ {result["score"]:.0%}</span>'
            
            st.markdown(tags_html, unsafe_allow_html=True)
            
            # Extrait avec highlights
            if result.get('highlights'):
                for highlight in result['highlights'][:2]:
                    st.markdown(f"ğŸ“Œ *...{highlight}...*")
            else:
                st.text(truncate_text(result.get('content', ''), 200))
        
        with col2:
            if st.button("ğŸ‘ï¸ Voir", key=f"view_{index}", use_container_width=True):
                show_document_detail(result)
        
        with col3:
            if st.button("ğŸ“¥ Utiliser", key=f"use_{index}", use_container_width=True):
                use_document_in_context(result)
        
        st.markdown("</div>", unsafe_allow_html=True)

def get_available_ai_models():
    """RÃ©cupÃ¨re les modÃ¨les IA disponibles avec leurs mÃ©tadonnÃ©es"""
    
    # Configuration des modÃ¨les avec icÃ´nes et mÃ©tadonnÃ©es
    models_config = [
        {
            'id': 'claude-3-opus',
            'name': 'Claude 3 Opus',
            'icon': 'ğŸ­',
            'description': 'Le plus puissant pour les tÃ¢ches complexes',
            'speed': 'âš¡âš¡',
            'quality': 'â­â­â­â­â­',
            'default': True
        },
        {
            'id': 'claude-3-sonnet',
            'name': 'Claude 3 Sonnet',
            'icon': 'ğŸµ',
            'description': 'Ã‰quilibre parfait entre vitesse et qualitÃ©',
            'speed': 'âš¡âš¡âš¡',
            'quality': 'â­â­â­â­',
            'default': True
        },
        {
            'id': 'gpt-4-turbo',
            'name': 'GPT-4 Turbo',
            'icon': 'ğŸš€',
            'description': 'Excellent pour la crÃ©ativitÃ© et l\'analyse',
            'speed': 'âš¡âš¡',
            'quality': 'â­â­â­â­â­',
            'default': False
        },
        {
            'id': 'gpt-3.5-turbo',
            'name': 'GPT-3.5 Turbo',
            'icon': 'âš¡',
            'description': 'Rapide et efficace pour les tÃ¢ches simples',
            'speed': 'âš¡âš¡âš¡âš¡',
            'quality': 'â­â­â­',
            'default': False
        },
        {
            'id': 'mistral-large',
            'name': 'Mistral Large',
            'icon': 'ğŸŒªï¸',
            'description': 'SpÃ©cialisÃ© dans le raisonnement juridique',
            'speed': 'âš¡âš¡âš¡',
            'quality': 'â­â­â­â­',
            'default': False
        }
    ]
    
    # Filtrer selon les modÃ¨les rÃ©ellement disponibles
    if _modules_cache.get('api_utils', {}).get('available'):
        available = _modules_cache['api_utils']['get_available_models']()
        models_config = [m for m in models_config if m['id'] in available]
    
    return models_config

def generate_with_multi_ai(doc_type, complexity, models, config):
    """GÃ©nÃ¨re un document avec plusieurs IA"""
    
    with st.spinner("ğŸš€ GÃ©nÃ©ration en cours avec plusieurs IA..."):
        # Container pour les rÃ©sultats
        results_container = st.container()
        
        # Progress tracking
        progress = st.progress(0)
        status = st.empty()
        
        results = {}
        
        for idx, model_id in enumerate(models):
            status.text(f"ğŸ¤– GÃ©nÃ©ration avec {model_id}...")
            progress.progress((idx + 1) / len(models))
            
            # Simuler la gÃ©nÃ©ration (remplacer par l'appel rÃ©el)
            time.sleep(1)
            
            # Stocker le rÃ©sultat
            results[model_id] = f"Contenu gÃ©nÃ©rÃ© par {model_id}"
        
        # Fusion des rÃ©sultats selon le mode
        status.text("ğŸ§¬ Fusion des rÃ©sultats...")
        
        if config['fusion_mode'] == "ğŸ§¬ SynthÃ¨se IA":
            final_result = ai_synthesis_fusion(results)
        elif config['fusion_mode'] == "ğŸ† Meilleur rÃ©sultat":
            final_result = select_best_result(results)
        else:
            final_result = concatenate_results(results)
        
        # Afficher le rÃ©sultat final
        progress.empty()
        status.empty()
        
        with results_container:
            st.success("âœ… GÃ©nÃ©ration terminÃ©e!")
            
            # Afficher le document gÃ©nÃ©rÃ©
            st.text_area(
                "ğŸ“„ Document gÃ©nÃ©rÃ©",
                value=final_result,
                height=400,
                key="generated_document_display"
            )
            
            # Actions
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.download_button(
                    "ğŸ’¾ TÃ©lÃ©charger",
                    final_result,
                    file_name=f"document_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                    mime="text/plain"
                )
            
            with col2:
                if st.button("ğŸ“Š Voir dÃ©tails"):
                    show_generation_details(results, config)
            
            with col3:
                if st.button("ğŸ”„ RegÃ©nÃ©rer"):
                    st.rerun()

def run_multi_ai_comparison(prompt, models, mode, metrics, temperature):
    """Lance une comparaison multi-IA"""
    
    results_container = st.container()
    
    with st.spinner("ğŸ¤– Comparaison en cours..."):
        # Animation de progression
        progress = st.progress(0)
        
        # Container pour les rÃ©sultats en temps rÃ©el
        live_results = {}
        
        # Colonnes pour afficher les rÃ©sultats en parallÃ¨le
        cols = st.columns(len(models))
        
        for idx, model in enumerate(models):
            with cols[idx]:
                st.markdown(f"### {model['icon']} {model['name']}")
                
                # Placeholder pour le rÃ©sultat
                result_placeholder = st.empty()
                
                # Simuler la gÃ©nÃ©ration
                with result_placeholder.container():
                    st.info("â³ En cours...")
                
                # Attendre un peu (remplacer par l'appel API rÃ©el)
                time.sleep(1 + idx * 0.5)
                
                # Afficher le rÃ©sultat
                with result_placeholder.container():
                    result_text = f"RÃ©ponse de {model['name']} pour: {prompt[:50]}..."
                    st.success("âœ… TerminÃ©")
                    st.text_area(
                        "RÃ©sultat",
                        value=result_text,
                        height=200,
                        key=f"result_{model['id']}"
                    )
                
                # MÃ©triques
                if metrics:
                    st.markdown("**Scores:**")
                    for metric in metrics:
                        score = 0.7 + (idx * 0.05)  # Simulation
                        st.progress(score)
                        st.caption(f"{metric}: {score:.0%}")
                
                live_results[model['id']] = {
                    'content': result_text,
                    'scores': {m: 0.7 + (idx * 0.05) for m in metrics}
                }
            
            progress.progress((idx + 1) / len(models))
        
        # Analyse comparative
        st.markdown("---")
        st.markdown("### ğŸ“Š Analyse comparative")
        
        # Graphique de comparaison
        if metrics:
            comparison_data = []
            for model_id, result in live_results.items():
                for metric, score in result['scores'].items():
                    comparison_data.append({
                        'ModÃ¨le': model_id,
                        'MÃ©trique': metric,
                        'Score': score
                    })
            
            df = pd.DataFrame(comparison_data)
            
            # Pivot pour affichage
            pivot_df = df.pivot(index='ModÃ¨le', columns='MÃ©trique', values='Score')
            st.dataframe(pivot_df.style.format("{:.0%}"), use_container_width=True)
        
        # Recommandation
        best_model = max(live_results.items(), 
                        key=lambda x: sum(x[1]['scores'].values()))[0]
        
        st.success(f"ğŸ† Meilleur modÃ¨le selon les critÃ¨res : **{best_model}**")
        
        # Actions finales
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ’¾ Sauvegarder la comparaison"):
                save_comparison_results(live_results, prompt, metrics)
        
        with col2:
            if st.button("ğŸ“¤ Exporter les rÃ©sultats"):
                export_comparison_results(live_results)
        
        with col3:
            if st.button("ğŸ”„ Nouvelle comparaison"):
                st.rerun()

def show_enhanced_footer():
    """Footer avec informations et liens utiles"""
    
    st.markdown("---")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown("**ğŸš€ Performance**")
        if 'unified_interface' in st.session_state:
            interface = st.session_state.unified_interface
            st.caption(f"Cache: {len(getattr(interface, 'cache', {}))} entrÃ©es")
            st.caption(f"RequÃªtes: {st.session_state.get('query_count', 0)}")
    
    with col2:
        st.markdown("**ğŸ“Š Statistiques**")
        docs_count = len(st.session_state.get('azure_documents', {}))
        st.caption(f"Documents: {docs_count}")
        st.caption(f"Analyses: {st.session_state.get('analysis_count', 0)}")
    
    with col3:
        st.markdown("**ğŸ”§ Modules actifs**")
        active_modules = sum(1 for v in _modules_cache.values() if v.get('available'))
        st.caption(f"Modules: {active_modules}/{len(_modules_cache)}")
        if st.button("Voir dÃ©tails", key="show_modules_footer"):
            show_modules_status_dialog()
    
    with col4:
        st.markdown("**â„¹ï¸ Aide**")
        if st.button("ğŸ“– Guide", key="show_guide"):
            show_user_guide()
        if st.button("ğŸ’¬ Support", key="show_support"):
            st.info("Support: support@nexora-law.ai")

# ========================= FONCTIONS UTILITAIRES =========================

def handle_quick_action(action: str):
    """GÃ¨re les actions rapides"""
    
    action_map = {
        "ğŸ“ RÃ©daction": "rÃ©diger ",
        "ğŸ¤– Analyse IA": "analyser ",
        "ğŸ” Recherche": "rechercher ",
        "ğŸ“Š SynthÃ¨se": "synthÃ©tiser ",
        "ğŸ“‹ Bordereau": "crÃ©er bordereau ",
        "âš–ï¸ Juridique": "plainte "
    }
    
    if action in action_map:
        st.session_state.pending_query = action_map[action]
        st.session_state.active_tab = 0  # Retour Ã  l'onglet recherche
        st.rerun()

def is_recent_document(doc: dict) -> bool:
    """VÃ©rifie si un document est rÃ©cent"""
    if not doc.get('date'):
        return False
    
    try:
        doc_date = datetime.fromisoformat(str(doc['date']))
        days_old = (datetime.now() - doc_date).days
        return days_old < 30
    except:
        return False

def filter_results(results: list, categories: list, min_score: int) -> list:
    """Filtre les rÃ©sultats selon les critÃ¨res"""
    filtered = results
    
    if categories:
        filtered = [r for r in filtered if r.get('category') in categories]
    
    if min_score > 0:
        filtered = [r for r in filtered if r.get('score', 0) * 100 >= min_score]
    
    return filtered

def sort_results(results: list, sort_by: str) -> list:
    """Trie les rÃ©sultats selon le critÃ¨re"""
    if sort_by == "Pertinence":
        return sorted(results, key=lambda x: x.get('score', 0), reverse=True)
    elif sort_by == "Date":
        return sorted(results, key=lambda x: x.get('date', ''), reverse=True)
    elif sort_by == "Titre":
        return sorted(results, key=lambda x: x.get('title', ''))
    elif sort_by == "CatÃ©gorie":
        return sorted(results, key=lambda x: x.get('category', ''))
    return results

def ai_synthesis_fusion(results: dict) -> str:
    """Fusionne les rÃ©sultats avec une synthÃ¨se IA"""
    # Simulation - remplacer par l'appel rÃ©el Ã  l'IA
    synthesis = "SYNTHÃˆSE DES GÃ‰NÃ‰RATIONS MULTI-IA\n\n"
    synthesis += "Les diffÃ©rents modÃ¨les ont gÃ©nÃ©rÃ© des contenus complÃ©mentaires:\n\n"
    
    for model, content in results.items():
        synthesis += f"- {model}: Points clÃ©s extraits\n"
    
    synthesis += "\nSYNTHÃˆSE FINALE:\n"
    synthesis += "Contenu fusionnÃ© et optimisÃ© par l'IA..."
    
    return synthesis

def select_best_result(results: dict) -> str:
    """SÃ©lectionne le meilleur rÃ©sultat selon des critÃ¨res"""
    # Simulation - implÃ©menter la logique rÃ©elle
    best_model = list(results.keys())[0]
    return f"MEILLEUR RÃ‰SULTAT (sÃ©lectionnÃ©: {best_model}):\n\n{results[best_model]}"

def concatenate_results(results: dict) -> str:
    """ConcatÃ¨ne simplement les rÃ©sultats"""
    concatenated = "RÃ‰SULTATS CONCATÃ‰NÃ‰S:\n\n"
    
    for model, content in results.items():
        concatenated += f"=== {model} ===\n{content}\n\n"
    
    return concatenated

def save_comparison_results(results: dict, prompt: str, metrics: list):
    """Sauvegarde les rÃ©sultats de comparaison"""
    st.success("âœ… Comparaison sauvegardÃ©e!")
    
    # ImplÃ©menter la logique de sauvegarde
    st.session_state.saved_comparisons = st.session_state.get('saved_comparisons', [])
    st.session_state.saved_comparisons.append({
        'timestamp': datetime.now(),
        'prompt': prompt,
        'results': results,
        'metrics': metrics
    })

def export_comparison_results(results: dict):
    """Exporte les rÃ©sultats de comparaison"""
    # CrÃ©er un rapport
    report = "RAPPORT DE COMPARAISON MULTI-IA\n"
    report += f"Date: {datetime.now().strftime('%d/%m/%Y %H:%M')}\n\n"
    
    for model, data in results.items():
        report += f"=== {model} ===\n"
        report += f"Contenu: {data['content'][:200]}...\n"
        report += "Scores:\n"
        for metric, score in data['scores'].items():
            report += f"  - {metric}: {score:.0%}\n"
        report += "\n"
    
    st.download_button(
        "ğŸ’¾ TÃ©lÃ©charger le rapport",
        report,
        file_name=f"comparaison_ia_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
        mime="text/plain"
    )

def show_modules_status_dialog():
    """Affiche le statut dÃ©taillÃ© des modules"""
    with st.expander("ğŸ”§ Ã‰tat dÃ©taillÃ© des modules", expanded=True):
        for module_name, module_data in _modules_cache.items():
            if module_data.get('available'):
                st.success(f"âœ… {module_name}")
            else:
                st.error(f"âŒ {module_name}")

def show_user_guide():
    """Affiche le guide utilisateur"""
    with st.expander("ğŸ“– Guide d'utilisation", expanded=True):
        st.markdown("""
        ### ğŸš€ DÃ©marrage rapide
        
        1. **Recherche naturelle** : Ã‰crivez ce que vous voulez faire
        2. **Actions rapides** : Utilisez les boutons pour des actions courantes
        3. **Multi-IA** : Comparez les rÃ©ponses de plusieurs IA
        4. **GÃ©nÃ©ration** : CrÃ©ez des documents juridiques complets
        
        ### ğŸ’¡ Conseils
        
        - Utilisez @ pour rÃ©fÃ©rencer un dossier (ex: @Martin)
        - Activez le mode Multi-IA pour des rÃ©sultats optimaux
        - Les documents longs utilisent automatiquement le module appropriÃ©
        
        ### ğŸ¯ Exemples de requÃªtes
        
        - "Analyser les risques dans l'affaire Martin"
        - "RÃ©diger une plainte exhaustive contre Vinci"
        - "PrÃ©parer mon client pour l'audience de demain"
        - "SynthÃ©tiser tous les Ã©changes avec l'avocat adverse"
        """)

# ========================= CLASSES ET INTERFACES PRINCIPALES =========================

class UnifiedSearchAnalysisInterface:
    """Interface unifiÃ©e pour la recherche et l'analyse avec IA"""
    
    def __init__(self):
        """Initialisation avec chargement lazy des composants"""
        self.search_service = None
        self.nl_analyzer = None
        self.llm_manager = None
        self.jurisprudence_verifier = None
        self.cache = {}
        
        # Initialiser les composants de maniÃ¨re lazy
        self._init_components_lazy()
    
    def _init_components_lazy(self):
        """Initialise les composants uniquement quand nÃ©cessaire"""
        # Les composants seront chargÃ©s Ã  la demande
        pass
    
    def get_search_service(self):
        """RÃ©cupÃ¨re le service de recherche (lazy loading)"""
        if not self.search_service:
            search_module = get_module('search_service')
            if search_module.get('available'):
                self.search_service = search_module['get_universal_search_service']()
        return self.search_service
    
    def get_nl_analyzer(self):
        """RÃ©cupÃ¨re l'analyseur de langage naturel (lazy loading)"""
        if not self.nl_analyzer:
            self.nl_analyzer = NaturalLanguageAnalyzer()
        return self.nl_analyzer
    
    async def process_universal_query(self, query: str):
        """Traite une requÃªte universelle avec IA"""
        
        # IncrÃ©menter le compteur
        st.session_state.query_count = st.session_state.get('query_count', 0) + 1
        
        # Sauvegarder la requÃªte
        st.session_state.last_universal_query = query
        
        # Analyse en langage naturel
        nl_analyzer = self.get_nl_analyzer()
        nl_analysis = await nl_analyzer.analyze_natural_query(query)
        st.session_state.nl_analysis = nl_analysis
        
        # Afficher l'analyse si demandÃ©
        if st.session_state.get('show_nl_analysis', False):
            with st.expander("ğŸ§  Analyse IA de votre requÃªte", expanded=True):
                st.json(nl_analysis)
        
        # Routage intelligent selon l'intention
        await self.route_by_intention(query, nl_analysis)
    
    async def route_by_intention(self, query: str, nl_analysis: dict):
        """Route la requÃªte selon l'intention dÃ©tectÃ©e"""
        
        intention = nl_analysis.get('intention', 'search')
        
        # Map des intentions vers les handlers
        intention_handlers = {
            'analyze': self._process_analysis_request,
            'create': self._process_create_request,
            'prepare': self._process_preparation_request,
            'synthesize': self._process_synthesis_request,
            'search': self._process_search_request
        }
        
        handler = intention_handlers.get(intention, self._process_search_request)
        await handler(query, nl_analysis)
    
    async def _process_analysis_request(self, query: str, nl_analysis: dict):
        """Traite une demande d'analyse"""
        st.session_state.analysis_context = nl_analysis
        st.session_state.analysis_count = st.session_state.get('analysis_count', 0) + 1
        
        # Rediriger vers l'interface d'analyse
        await self.show_analyse_ia_interface(nl_analysis)
    
    async def _process_create_request(self, query: str, nl_analysis: dict):
        """Traite une demande de crÃ©ation de document"""
        
        # VÃ©rifier la complexitÃ© du document
        if nl_analysis.get('document_complexite') in ['exhaustif', 'long']:
            # Documents longs
            if get_module('generation_longue', 'available'):
                st.info("ğŸ“œ Redirection vers le module de gÃ©nÃ©ration longue...")
                st.session_state.show_generation_longue = True
                st.session_state.juridique_context = nl_analysis
                st.rerun()
                return
        
        # Documents standards
        st.session_state.generation_context = nl_analysis
        await self.generate_standard_document(nl_analysis)
    
    async def _process_preparation_request(self, query: str, nl_analysis: dict):
        """Traite une demande de prÃ©paration"""
        st.info("ğŸ“‹ Module de prÃ©paration client")
        # ImplÃ©menter la logique de prÃ©paration
    
    async def _process_synthesis_request(self, query: str, nl_analysis: dict):
        """Traite une demande de synthÃ¨se"""
        st.info("ğŸ“‘ GÃ©nÃ©ration de synthÃ¨se")
        # ImplÃ©menter la logique de synthÃ¨se
    
    async def _process_search_request(self, query: str, nl_analysis: dict):
        """Traite une demande de recherche"""
        
        # Enrichir la requÃªte
        enhanced_query = self._enhance_query_from_nl(query, nl_analysis)
        
        # Effectuer la recherche
        search_service = self.get_search_service()
        if search_service:
            results = await search_service.search(enhanced_query)
            st.session_state.search_results = results.documents if hasattr(results, 'documents') else results
            
            if results:
                st.success(f"âœ… {len(st.session_state.search_results)} rÃ©sultats trouvÃ©s")
            else:
                st.warning("âš ï¸ Aucun rÃ©sultat trouvÃ©")
        else:
            st.error("âŒ Service de recherche non disponible")
    
    def _enhance_query_from_nl(self, query: str, nl_analysis: dict) -> str:
        """Enrichit la requÃªte avec l'analyse NL"""
        
        enhanced = nl_analysis.get('requete_reformulee', query)
        
        # Ajouter la rÃ©fÃ©rence si prÃ©sente
        if nl_analysis.get('reference_dossier') and '@' not in enhanced:
            enhanced = f"@{nl_analysis['reference_dossier']} {enhanced}"
        
        # Ajouter les mots-clÃ©s importants
        if nl_analysis.get('mots_cles_importants'):
            for mot in nl_analysis['mots_cles_importants']:
                if mot.lower() not in enhanced.lower():
                    enhanced += f" {mot}"
        
        return enhanced
    
    async def show_analyse_ia_interface(self, nl_analysis: dict):
        """Interface d'analyse IA"""
        
        st.header("ğŸ¤– Analyse IA des documents")
        
        # Configuration de l'analyse dans un container Ã©lÃ©gant
        with st.container():
            st.markdown("""
            <div class="result-card">
            """, unsafe_allow_html=True)
            
            # Configuration en colonnes
            col1, col2 = st.columns(2)
            
            with col1:
                infraction = st.text_input(
                    "ğŸ¯ Type d'infraction",
                    value=nl_analysis.get('elements_juridiques', [''])[0] if nl_analysis.get('elements_juridiques') else '',
                    placeholder="Ex: Abus de biens sociaux"
                )
                
                client_nom = st.text_input(
                    "ğŸ‘¤ Nom du client",
                    value=nl_analysis.get('reference_dossier', ''),
                    placeholder="Personne physique ou morale"
                )
            
            with col2:
                # SÃ©lection des modÃ¨les IA
                models = get_available_ai_models()
                selected_models = st.multiselect(
                    "ğŸ¤– ModÃ¨les IA",
                    [m['name'] for m in models],
                    default=[models[0]['name']] if models else []
                )
                
                fusion_mode = st.radio(
                    "ğŸ§¬ Mode de fusion",
                    ["SynthÃ¨se IA", "Meilleur rÃ©sultat", "Consensus"],
                    horizontal=True
                )
            
            st.markdown("</div>", unsafe_allow_html=True)
        
        # SÃ©lection des analyses
        st.markdown("### ğŸ¯ Types d'analyses")
        
        analysis_types = list(get_module('config', 'ANALYSIS_PROMPTS_AFFAIRES').keys())
        selected_analyses = st.multiselect(
            "SÃ©lectionnez les analyses",
            analysis_types,
            default=analysis_types[:2] if len(analysis_types) >= 2 else analysis_types
        )
        
        # Bouton d'analyse avec style
        if st.button("ğŸš€ Lancer l'analyse IA", type="primary", use_container_width=True):
            if infraction and client_nom and selected_models and selected_analyses:
                await self.run_ai_analysis(
                    infraction=infraction,
                    client_nom=client_nom,
                    models=selected_models,
                    analyses=selected_analyses,
                    fusion_mode=fusion_mode
                )
            else:
                st.error("âŒ Veuillez remplir tous les champs")
    
    async def run_ai_analysis(self, infraction: str, client_nom: str, 
                             models: list, analyses: list, fusion_mode: str):
        """Lance l'analyse IA avec les paramÃ¨tres"""
        
        with st.spinner("ğŸ¤– Analyse IA en cours..."):
            # Simulation - remplacer par les appels rÃ©els
            results = {}
            
            progress = st.progress(0)
            for idx, analysis_type in enumerate(analyses):
                results[analysis_type] = f"RÃ©sultat de l'analyse {analysis_type}"
                progress.progress((idx + 1) / len(analyses))
            
            # Stocker les rÃ©sultats
            st.session_state.analysis_results = results
            
            # Afficher les rÃ©sultats
            st.success("âœ… Analyse terminÃ©e!")
            
            for analysis_type, result in results.items():
                with st.expander(analysis_type, expanded=True):
                    st.markdown(result)
    
    async def generate_standard_document(self, nl_analysis: dict):
        """GÃ©nÃ¨re un document standard"""
        
        doc_type = nl_analysis.get('type_document', 'document')
        
        st.info(f"ğŸ“ GÃ©nÃ©ration d'un {doc_type} en cours...")
        
        # Simulation de gÃ©nÃ©ration
        with st.spinner("GÃ©nÃ©ration..."):
            time.sleep(2)
            
            content = f"""DOCUMENT GÃ‰NÃ‰RÃ‰
Type: {doc_type}
Date: {datetime.now().strftime('%d/%m/%Y')}
RÃ©fÃ©rence: {nl_analysis.get('reference_dossier', 'REF')}

[Contenu du document gÃ©nÃ©rÃ© par l'IA]
"""
            
            st.session_state.generated_content = content
            
            # Afficher le rÃ©sultat
            st.text_area(
                "Document gÃ©nÃ©rÃ©",
                value=content,
                height=400
            )

class NaturalLanguageAnalyzer:
    """Analyseur de langage naturel pour comprendre les requÃªtes"""
    
    def __init__(self):
        self.llm_manager = None
        self._init_llm_lazy()
    
    def _init_llm_lazy(self):
        """Initialise le LLM de maniÃ¨re lazy"""
        # Le LLM sera chargÃ© uniquement si nÃ©cessaire
        pass
    
    async def analyze_natural_query(self, query: str) -> dict:
        """Analyse une requÃªte en langage naturel"""
        
        # Pour la dÃ©mo, utiliser une analyse basique
        # Remplacer par l'appel rÃ©el Ã  l'IA
        return self._fallback_analysis(query)
    
    def _fallback_analysis(self, query: str) -> dict:
        """Analyse de fallback basÃ©e sur des rÃ¨gles"""
        
        query_lower = query.lower()
        
        # DÃ©tection de l'intention
        intention = "search"
        if any(word in query_lower for word in ['analyser', 'analyse']):
            intention = "analyze"
        elif any(word in query_lower for word in ['rÃ©diger', 'crÃ©er', 'gÃ©nÃ©rer']):
            intention = "create"
        elif any(word in query_lower for word in ['prÃ©parer', 'prÃ©paration']):
            intention = "prepare"
        elif any(word in query_lower for word in ['synthÃ¨se', 'synthÃ©tiser']):
            intention = "synthesize"
        
        # DÃ©tection du type de document
        type_document = None
        if 'plainte' in query_lower:
            type_document = 'plainte'
        elif 'conclusions' in query_lower:
            type_document = 'conclusions'
        elif 'mÃ©moire' in query_lower:
            type_document = 'mÃ©moire'
        
        # DÃ©tection de la complexitÃ©
        document_complexite = "standard"
        if any(word in query_lower for word in ['exhaustif', 'complet', 'long']):
            document_complexite = "exhaustif"
        
        # Extraction de la rÃ©fÃ©rence
        reference = None
        ref_match = re.search(r'@(\w+)', query)
        if ref_match:
            reference = ref_match.group(1)
        
        return {
            "intention": intention,
            "action_principale": f"{intention} detected",
            "reference_dossier": reference,
            "type_document": type_document,
            "document_complexite": document_complexite,
            "longueur_estimee": "longue" if document_complexite == "exhaustif" else "moyenne",
            "parties": {"demandeurs": [], "defendeurs": []},
            "contexte": "normal",
            "elements_juridiques": [],
            "contraintes_temporelles": None,
            "mots_cles_importants": query.split()[:5],
            "indicateurs_complexite": [],
            "requete_reformulee": query
        }

# ========================= FONCTIONS HELPER =========================

def clear_all_results():
    """Efface tous les rÃ©sultats"""
    keys_to_clear = [
        'search_results', 'analysis_results', 'generated_content',
        'multi_ai_results', 'synthesis_result', 'nl_analysis'
    ]
    
    for key in keys_to_clear:
        if key in st.session_state:
            del st.session_state[key]

def save_all_results():
    """Sauvegarde tous les rÃ©sultats"""
    
    results = {
        'timestamp': datetime.now().isoformat(),
        'search_results': st.session_state.get('search_results'),
        'analysis_results': st.session_state.get('analysis_results'),
        'generated_content': st.session_state.get('generated_content'),
        'multi_ai_results': st.session_state.get('multi_ai_results')
    }
    
    # CrÃ©er un fichier JSON
    import json
    json_str = json.dumps(results, indent=2, ensure_ascii=False, default=str)
    
    st.download_button(
        "ğŸ’¾ TÃ©lÃ©charger tous les rÃ©sultats",
        json_str,
        file_name=f"resultats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        mime="application/json"
    )

def export_results_dialog():
    """Dialog d'export des rÃ©sultats"""
    with st.expander("ğŸ“¤ Options d'export", expanded=True):
        format_export = st.selectbox(
            "Format",
            ["JSON", "PDF", "Word", "Excel"]
        )
        
        if st.button("Exporter", type="primary"):
            st.success(f"âœ… Export en {format_export} en cours...")

def share_results_dialog():
    """Dialog de partage des rÃ©sultats"""
    with st.expander("ğŸ”— Partager les rÃ©sultats", expanded=True):
        share_link = f"https://nexora-law.ai/share/{datetime.now().timestamp()}"
        st.code(share_link)
        
        if st.button("ğŸ“‹ Copier le lien"):
            st.success("âœ… Lien copiÃ©!")

def show_document_detail(document: dict):
    """Affiche le dÃ©tail d'un document"""
    with st.expander(f"ğŸ“„ {document.get('title', 'Document')}", expanded=True):
        st.markdown(f"**CatÃ©gorie:** {document.get('category', 'Non catÃ©gorisÃ©')}")
        st.markdown(f"**Date:** {format_legal_date(document.get('date', ''))}")
        st.markdown("---")
        st.text(document.get('content', 'Contenu non disponible'))

def use_document_in_context(document: dict):
    """Utilise un document dans le contexte actuel"""
    st.session_state.selected_document = document
    st.success(f"âœ… Document '{document.get('title', 'Sans titre')}' sÃ©lectionnÃ©")

def show_search_results_enhanced():
    """Affiche les rÃ©sultats de recherche amÃ©liorÃ©s"""
    results = st.session_state.get('search_results', [])
    
    if not results:
        st.info("Aucun rÃ©sultat de recherche")
        return
    
    st.markdown(f"### ğŸ” {len(results)} rÃ©sultats trouvÃ©s")
    
    for idx, result in enumerate(results[:10]):
        display_result_card(result, idx)

def show_analysis_results_enhanced():
    """Affiche les rÃ©sultats d'analyse amÃ©liorÃ©s"""
    results = st.session_state.get('analysis_results', {})
    
    if not results:
        st.info("Aucun rÃ©sultat d'analyse")
        return
    
    for analysis_type, content in results.items():
        with st.expander(analysis_type, expanded=True):
            st.markdown(content)

def show_generation_results_enhanced():
    """Affiche les rÃ©sultats de gÃ©nÃ©ration amÃ©liorÃ©s"""
    content = st.session_state.get('generated_content', '')
    
    if not content:
        st.info("Aucun contenu gÃ©nÃ©rÃ©")
        return
    
    st.text_area(
        "Document gÃ©nÃ©rÃ©",
        value=content,
        height=400
    )

def show_multi_ai_results_enhanced():
    """Affiche les rÃ©sultats multi-IA amÃ©liorÃ©s"""
    results = st.session_state.get('multi_ai_results', {})
    
    if not results:
        st.info("Aucun rÃ©sultat multi-IA")
        return
    
    for model, data in results.items():
        with st.expander(f"ğŸ¤– {model}", expanded=True):
            st.markdown(data.get('content', ''))

def show_work_statistics_enhanced():
    """Affiche les statistiques de travail amÃ©liorÃ©es"""
    
    # MÃ©triques principales
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "ğŸ“„ Documents",
            len(st.session_state.get('azure_documents', {}))
        )
    
    with col2:
        st.metric(
            "ğŸ” Recherches",
            st.session_state.get('query_count', 0)
        )
    
    with col3:
        st.metric(
            "ğŸ“Š Analyses",
            st.session_state.get('analysis_count', 0)
        )
    
    with col4:
        st.metric(
            "ğŸ“ GÃ©nÃ©rations",
            st.session_state.get('generation_count', 0)
        )
    
    # Graphiques
    st.markdown("### ğŸ“ˆ Ã‰volution de l'activitÃ©")
    st.info("Graphiques d'activitÃ© Ã  implÃ©menter")

def show_advanced_search_interface():
    """Interface de recherche avancÃ©e"""
    
    st.markdown("### ğŸ” Recherche avancÃ©e")
    
    # Constructeur de requÃªte
    col1, col2 = st.columns(2)
    
    with col1:
        search_field = st.selectbox(
            "Champ",
            ["Tous", "Titre", "Contenu", "MÃ©tadonnÃ©es"]
        )
        
        operator = st.selectbox(
            "OpÃ©rateur",
            ["Contient", "Ã‰gal Ã ", "Commence par", "Finit par"]
        )
    
    with col2:
        search_value = st.text_input(
            "Valeur",
            placeholder="Terme de recherche"
        )
        
        add_condition = st.button("â• Ajouter condition")
    
    # Affichage des conditions
    if 'search_conditions' not in st.session_state:
        st.session_state.search_conditions = []
    
    if add_condition and search_value:
        st.session_state.search_conditions.append({
            'field': search_field,
            'operator': operator,
            'value': search_value
        })
    
    if st.session_state.search_conditions:
        st.markdown("**Conditions de recherche:**")
        for idx, condition in enumerate(st.session_state.search_conditions):
            col1, col2 = st.columns([4, 1])
            with col1:
                st.text(f"{condition['field']} {condition['operator']} '{condition['value']}'")
            with col2:
                if st.button("âŒ", key=f"remove_cond_{idx}"):
                    st.session_state.search_conditions.pop(idx)
                    st.rerun()

def show_ai_fusion_interface():
    """Interface de fusion des rÃ©sultats IA"""
    
    st.markdown("### ğŸ§¬ Fusion des rÃ©sultats IA")
    
    st.info("Interface de fusion avancÃ©e Ã  implÃ©menter")

def show_analytics_dashboard():
    """Tableau de bord analytique"""
    
    st.markdown("### ğŸ“ˆ Analytics")
    
    # KPIs
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric(
            "EfficacitÃ© moyenne",
            "87%",
            "+5%"
        )
    
    with col2:
        st.metric(
            "Temps moyen",
            "2.3s",
            "-0.5s"
        )
    
    with col3:
        st.metric(
            "Satisfaction",
            "4.8/5",
            "+0.2"
        )

def show_configuration_interface():
    """Interface de configuration"""
    
    st.markdown("### âš™ï¸ Configuration")
    
    # ParamÃ¨tres gÃ©nÃ©raux
    with st.expander("ParamÃ¨tres gÃ©nÃ©raux", expanded=True):
        st.slider("Limite de rÃ©sultats", 10, 100, 20)
        st.slider("Timeout API (secondes)", 5, 60, 30)
        st.checkbox("Mode debug", value=False)
    
    # ParamÃ¨tres IA
    with st.expander("ParamÃ¨tres IA", expanded=True):
        st.slider("TempÃ©rature par dÃ©faut", 0.0, 1.0, 0.3)
        st.number_input("Tokens maximum", 100, 8000, 2000)

def show_generation_details(results: dict, config: dict):
    """Affiche les dÃ©tails de gÃ©nÃ©ration"""
    
    with st.expander("ğŸ“Š DÃ©tails de gÃ©nÃ©ration", expanded=True):
        # Afficher les rÃ©sultats par modÃ¨le
        for model, content in results.items():
            st.markdown(f"**{model}**")
            st.text_area(
                "Contenu",
                value=content[:500] + "...",
                height=200,
                key=f"detail_{model}"
            )
        
        # Configuration utilisÃ©e
        st.markdown("**Configuration:**")
        st.json(config)

# ========================= EXPORT ET COMPATIBILITÃ‰ =========================

# Fonction pour compatibilitÃ© avec l'ancien systÃ¨me
def show_page():
    """Fonction de compatibilitÃ© pour l'ancien systÃ¨me"""
    run()

# Point d'entrÃ©e principal
if __name__ == "__main__":
    run()