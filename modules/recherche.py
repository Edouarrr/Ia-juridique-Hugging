# modules/recherche.py (EXTENSION POUR ANALYSES AVANC√âES)

# Ajouter ces imports en haut du fichier
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta
import networkx as nx
from collections import defaultdict, Counter
import json

# Modifier la fonction analyze_universal_query pour d√©tecter ces nouvelles demandes
def analyze_universal_query(query: str) -> dict:
    """Analyse la requ√™te pour d√©terminer l'intention (VERSION √âTENDUE)"""
    analysis = {
        'intent': 'search',  # search, analysis, redaction, timeline, mapping, comparison
        'type': 'simple',
        'has_reference': False,
        'reference': None,
        'action': None,
        'document_type': None,
        'subject_matter': None,
        'confidence': 0.0,
        'details': {}
    }
    
    query_lower = query.lower()
    
    # D√©tecter les r√©f√©rences
    if '@' in query:
        analysis['has_reference'] = True
        ref_match = re.search(r'@(\w+)', query)
        if ref_match:
            analysis['reference'] = ref_match.group(1)
    
    # NOUVELLES D√âTECTIONS D'ANALYSE AVANC√âE
    
    # Chronologie
    timeline_keywords = ['chronologie', 'timeline', 'calendrier', 's√©quence', 'd√©roulement', 'historique']
    if any(kw in query_lower for kw in timeline_keywords):
        analysis['intent'] = 'timeline'
        analysis['confidence'] = 0.9
        
        # Type de chronologie
        if any(kw in query_lower for kw in ['fait', '√©v√©nement', 'action']):
            analysis['details']['timeline_type'] = 'facts'
        elif any(kw in query_lower for kw in ['proc√©dure', 'proc√©dural', 'juridique', 'instance']):
            analysis['details']['timeline_type'] = 'procedure'
        else:
            analysis['details']['timeline_type'] = 'general'
    
    # Cartographie/Mapping
    mapping_keywords = ['cartographie', 'mapping', 'carte', 'r√©seau', 'liens', 'relations', 'connexions']
    if any(kw in query_lower for kw in mapping_keywords):
        analysis['intent'] = 'mapping'
        analysis['confidence'] = 0.9
        
        # Type d'entit√©s
        if any(kw in query_lower for kw in ['personne', 'individu', 'acteur']):
            analysis['details']['entity_type'] = 'persons'
        elif any(kw in query_lower for kw in ['soci√©t√©', 'entreprise', 'entit√©']):
            analysis['details']['entity_type'] = 'companies'
        else:
            analysis['details']['entity_type'] = 'all'
    
    # Comparaison
    comparison_keywords = ['comparer', 'comparaison', 'diff√©rence', 'convergence', '√©volution', 'contradiction']
    if any(kw in query_lower for kw in comparison_keywords):
        analysis['intent'] = 'comparison'
        analysis['confidence'] = 0.9
        
        # Type de comparaison
        if any(kw in query_lower for kw in ['audition', 'd√©claration', 't√©moignage']):
            analysis['details']['comparison_type'] = 'auditions'
        elif any(kw in query_lower for kw in ['version', 'r√©cit']):
            analysis['details']['comparison_type'] = 'versions'
        else:
            analysis['details']['comparison_type'] = 'general'
    
    # Garder les d√©tections pr√©c√©dentes (r√©daction, etc.)
    # [Code existant pour d√©tecter r√©daction...]
    
    return analysis

# Modifier process_universal_query pour traiter ces nouvelles intentions
def process_universal_query(query: str):
    """Traite la requ√™te universelle (VERSION √âTENDUE)"""
    analysis = analyze_universal_query(query)
    st.session_state.query_analysis = analysis
    
    with st.spinner("üîÑ Traitement de votre demande..."):
        
        # NOUVEAUX TYPES DE TRAITEMENT
        if analysis['intent'] == 'timeline':
            process_timeline_request(query, analysis)
            
        elif analysis['intent'] == 'mapping':
            process_mapping_request(query, analysis)
            
        elif analysis['intent'] == 'comparison':
            process_comparison_request(query, analysis)
            
        # Traitements existants
        elif analysis['intent'] == 'redaction':
            process_redaction_request(query, analysis)
            
        elif analysis['intent'] == 'analysis':
            process_analysis_request(query, analysis)
            
        else:  # search
            process_search_request(query, analysis)

# NOUVELLES FONCTIONS D'ANALYSE AVANC√âE

def process_timeline_request(query: str, analysis: dict):
    """Traite une demande de chronologie"""
    
    # 1. Collecter les documents pertinents
    documents = collect_timeline_documents(analysis)
    
    if not documents:
        st.warning("‚ö†Ô∏è Aucun document trouv√© pour cr√©er la chronologie")
        return
    
    # 2. Extraire les √©v√©nements avec l'IA
    timeline_type = analysis['details'].get('timeline_type', 'general')
    events = extract_timeline_events(documents, timeline_type)
    
    # 3. Cr√©er la visualisation
    timeline_viz = create_timeline_visualization(events, timeline_type)
    
    # 4. Stocker les r√©sultats
    st.session_state.timeline_result = {
        'events': events,
        'visualization': timeline_viz,
        'type': timeline_type,
        'document_count': len(documents),
        'timestamp': datetime.now()
    }

def collect_timeline_documents(analysis: dict) -> list:
    """Collecte les documents pour la chronologie"""
    documents = []
    
    # Si r√©f√©rence sp√©cifique
    if analysis['reference']:
        ref_results = search_by_reference(f"@{analysis['reference']}")
        documents.extend(ref_results)
    
    # Sinon chercher tous les documents avec dates
    else:
        # Rechercher dans les documents locaux
        for doc_id, doc in st.session_state.get('azure_documents', {}).items():
            # V√©rifier si le document contient des dates
            if has_temporal_information(doc.content):
                documents.append({
                    'id': doc_id,
                    'title': doc.title,
                    'content': doc.content,
                    'source': doc.source
                })
    
    return documents

def has_temporal_information(content: str) -> bool:
    """V√©rifie si le contenu contient des informations temporelles"""
    # Patterns de dates
    date_patterns = [
        r'\d{1,2}[/-]\d{1,2}[/-]\d{2,4}',  # 01/01/2024
        r'\d{1,2}\s+(janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\s+\d{2,4}',
        r'(lundi|mardi|mercredi|jeudi|vendredi|samedi|dimanche)',
        r'(hier|aujourd\'hui|demain)',
        r'(semaine|mois|ann√©e)\s+(dernier|derni√®re|prochain|prochaine)'
    ]
    
    for pattern in date_patterns:
        if re.search(pattern, content, re.IGNORECASE):
            return True
    
    return False

def extract_timeline_events(documents: list, timeline_type: str) -> list:
    """Extrait les √©v√©nements de la chronologie avec l'IA"""
    
    llm_manager = MultiLLMManager()
    if not llm_manager.clients:
        return []
    
    # Prompt sp√©cialis√© selon le type
    prompts = {
        'facts': """Extrais TOUS les faits et √©v√©nements dat√©s de ces documents.
Pour chaque fait, indique :
- Date exacte ou p√©riode
- Description pr√©cise du fait
- Personnes/entit√©s impliqu√©es
- Source/document d'origine
- Importance (1-5)

Format JSON : {"date": "YYYY-MM-DD", "description": "...", "actors": [...], "source": "...", "importance": N}""",
        
        'procedure': """Extrais TOUS les actes de proc√©dure et √©tapes juridiques dat√©s.
Pour chaque acte :
- Date exacte
- Type d'acte (plainte, audition, perquisition, mise en examen, etc.)
- Autorit√© concern√©e
- Personnes vis√©es
- R√©sultat/d√©cision

Format JSON : {"date": "YYYY-MM-DD", "type": "...", "description": "...", "authority": "...", "targets": [...]}""",
        
        'general': """Extrais TOUS les √©v√©nements importants avec leurs dates.
Format JSON : {"date": "YYYY-MM-DD", "description": "...", "category": "..."}"""
    }
    
    prompt = prompts.get(timeline_type, prompts['general'])
    
    # Traiter par batch de documents
    all_events = []
    
    for i in range(0, len(documents), 5):  # Batch de 5 documents
        batch = documents[i:i+5]
        
        # Construire le contexte
        context = "\n\n".join([
            f"=== DOCUMENT: {doc['title']} ===\n{doc['content'][:2000]}"
            for doc in batch
        ])
        
        # Requ√™te √† l'IA
        full_prompt = f"{prompt}\n\nDocuments √† analyser :\n{context}\n\nExtrais les √©v√©nements au format JSON demand√©."
        
        try:
            # Utiliser une seule IA pour la coh√©rence
            provider = list(llm_manager.clients.keys())[0]
            response = llm_manager.query_single_llm(
                provider,
                full_prompt,
                "Tu es un expert en analyse temporelle de documents juridiques."
            )
            
            if response['success']:
                # Parser la r√©ponse JSON
                events = parse_timeline_events(response['response'], batch)
                all_events.extend(events)
                
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erreur extraction √©v√©nements : {str(e)}")
    
    # Trier par date
    all_events.sort(key=lambda x: x.get('date', ''))
    
    return all_events

def parse_timeline_events(ai_response: str, source_docs: list) -> list:
    """Parse les √©v√©nements extraits par l'IA"""
    events = []
    
    try:
        # Extraire les objets JSON de la r√©ponse
        json_matches = re.findall(r'\{[^}]+\}', ai_response)
        
        for match in json_matches:
            try:
                event = json.loads(match)
                
                # Normaliser la date
                if 'date' in event:
                    event['date'] = normalize_date(event['date'])
                
                # Ajouter la source si non pr√©sente
                if 'source' not in event and source_docs:
                    event['source'] = source_docs[0]['title']
                
                events.append(event)
                
            except json.JSONDecodeError:
                continue
                
    except Exception as e:
        print(f"Erreur parsing √©v√©nements : {e}")
    
    return events

def normalize_date(date_str: str) -> str:
    """Normalise une date au format YYYY-MM-DD"""
    # Essayer diff√©rents formats
    formats = [
        '%Y-%m-%d',
        '%d/%m/%Y',
        '%d-%m-%Y',
        '%d %B %Y',
        '%d %b %Y'
    ]
    
    for fmt in formats:
        try:
            date = datetime.strptime(date_str, fmt)
            return date.strftime('%Y-%m-%d')
        except:
            continue
    
    # Si √©chec, retourner la date originale
    return date_str

def create_timeline_visualization(events: list, timeline_type: str):
    """Cr√©e la visualisation de la chronologie"""
    
    if not events:
        return None
    
    # Cr√©er un DataFrame
    df = pd.DataFrame(events)
    
    # S'assurer que les dates sont au bon format
    try:
        df['date'] = pd.to_datetime(df['date'])
    except:
        pass
    
    # Cr√©er la figure Plotly
    fig = go.Figure()
    
    # Grouper par cat√©gorie/type si disponible
    categories = df.get('category', df.get('type', ['√âv√©nement'] * len(df))).unique()
    colors = px.colors.qualitative.Set3[:len(categories)]
    
    for i, category in enumerate(categories):
        cat_events = df[df.get('category', df.get('type', '√âv√©nement')) == category]
        
        # Ajouter les points
        fig.add_trace(go.Scatter(
            x=cat_events['date'],
            y=[i] * len(cat_events),
            mode='markers+text',
            name=category,
            marker=dict(size=12, color=colors[i % len(colors)]),
            text=cat_events['description'].apply(lambda x: x[:50] + '...' if len(x) > 50 else x),
            textposition='top center',
            hovertemplate='<b>%{text}</b><br>Date: %{x}<br><extra></extra>'
        ))
    
    # Mise en forme
    fig.update_layout(
        title=f"Chronologie des {timeline_type}",
        xaxis_title="Date",
        yaxis_title="Cat√©gorie",
        height=600,
        hovermode='closest',
        showlegend=True
    )
    
    # Masquer l'axe Y si une seule cat√©gorie
    if len(categories) == 1:
        fig.update_yaxes(visible=False)
    
    return fig

def process_mapping_request(query: str, analysis: dict):
    """Traite une demande de cartographie des relations"""
    
    # 1. Collecter les documents
    documents = collect_mapping_documents(analysis)
    
    if not documents:
        st.warning("‚ö†Ô∏è Aucun document trouv√© pour cr√©er la cartographie")
        return
    
    # 2. Extraire les entit√©s et relations
    entity_type = analysis['details'].get('entity_type', 'all')
    entities, relations = extract_entities_and_relations(documents, entity_type)
    
    # 3. Cr√©er le graphe
    graph = create_network_graph(entities, relations)
    
    # 4. Analyser le r√©seau
    network_analysis = analyze_network(graph)
    
    # 5. Cr√©er la visualisation
    visualization = create_network_visualization(graph, network_analysis)
    
    # 6. Stocker les r√©sultats
    st.session_state.mapping_result = {
        'entities': entities,
        'relations': relations,
        'graph': graph,
        'analysis': network_analysis,
        'visualization': visualization,
        'type': entity_type,
        'timestamp': datetime.now()
    }

def extract_entities_and_relations(documents: list, entity_type: str) -> tuple:
    """Extrait les entit√©s et leurs relations avec l'IA"""
    
    llm_manager = MultiLLMManager()
    if not llm_manager.clients:
        return [], []
    
    # Prompt sp√©cialis√©
    prompts = {
        'persons': """Identifie TOUTES les personnes physiques et leurs relations.
Pour chaque personne : nom, r√¥le, fonction
Pour chaque relation : personne1, personne2, type de relation, description

Format JSON :
Entit√©s : {"name": "...", "role": "...", "type": "person"}
Relations : {"source": "...", "target": "...", "type": "...", "description": "..."}""",
        
        'companies': """Identifie TOUTES les soci√©t√©s/entit√©s morales et leurs relations.
Pour chaque soci√©t√© : nom, forme juridique, r√¥le
Pour chaque relation : soci√©t√©1, soci√©t√©2, type (filiale, actionnaire, client, etc.)

Format JSON :
Entit√©s : {"name": "...", "legal_form": "...", "type": "company"}
Relations : {"source": "...", "target": "...", "type": "...", "stake": "..."}""",
        
        'all': """Identifie TOUTES les entit√©s (personnes ET soci√©t√©s) et leurs relations.
Distingue bien personnes physiques et morales.
Indique tous les liens : hi√©rarchiques, capitalistiques, contractuels, familiaux.

Format JSON comme ci-dessus."""
    }
    
    prompt = prompts.get(entity_type, prompts['all'])
    
    all_entities = {}
    all_relations = []
    
    # Traiter les documents
    for doc in documents[:20]:  # Limiter
        context = f"=== DOCUMENT: {doc['title']} ===\n{doc['content'][:3000]}"
        
        full_prompt = f"{prompt}\n\nDocument √† analyser :\n{context}"
        
        try:
            provider = list(llm_manager.clients.keys())[0]
            response = llm_manager.query_single_llm(
                provider,
                full_prompt,
                "Tu es un expert en analyse de r√©seaux et relations d'affaires."
            )
            
            if response['success']:
                entities, relations = parse_entities_relations(response['response'])
                
                # Fusionner les entit√©s (√©viter doublons)
                for entity in entities:
                    key = entity['name'].lower()
                    if key not in all_entities:
                        all_entities[key] = entity
                    else:
                        # Enrichir l'entit√© existante
                        all_entities[key].update(entity)
                
                all_relations.extend(relations)
                
        except Exception as e:
            print(f"Erreur extraction entit√©s : {e}")
    
    return list(all_entities.values()), all_relations

def parse_entities_relations(ai_response: str) -> tuple:
    """Parse les entit√©s et relations extraites par l'IA"""
    entities = []
    relations = []
    
    try:
        # Chercher les sections Entit√©s et Relations
        entities_section = re.search(r'Entit√©s?\s*:?\s*\n(.*?)(?=Relations?|$)', ai_response, re.DOTALL | re.IGNORECASE)
        relations_section = re.search(r'Relations?\s*:?\s*\n(.*?)$', ai_response, re.DOTALL | re.IGNORECASE)
        
        # Parser les entit√©s
        if entities_section:
            json_matches = re.findall(r'\{[^}]+\}', entities_section.group(1))
            for match in json_matches:
                try:
                    entity = json.loads(match)
                    entities.append(entity)
                except:
                    continue
        
        # Parser les relations
        if relations_section:
            json_matches = re.findall(r'\{[^}]+\}', relations_section.group(1))
            for match in json_matches:
                try:
                    relation = json.loads(match)
                    relations.append(relation)
                except:
                    continue
                    
    except Exception as e:
        print(f"Erreur parsing entit√©s/relations : {e}")
    
    return entities, relations

def create_network_graph(entities: list, relations: list) -> nx.Graph:
    """Cr√©e le graphe NetworkX"""
    G = nx.Graph()
    
    # Ajouter les n≈ìuds
    for entity in entities:
        G.add_node(
            entity['name'],
            **entity  # Ajouter tous les attributs
        )
    
    # Ajouter les ar√™tes
    for relation in relations:
        if relation['source'] in G and relation['target'] in G:
            G.add_edge(
                relation['source'],
                relation['target'],
                **relation  # Ajouter tous les attributs
            )
    
    return G

def analyze_network(G: nx.Graph) -> dict:
    """Analyse le r√©seau (centralit√©, clusters, etc.)"""
    analysis = {
        'node_count': G.number_of_nodes(),
        'edge_count': G.number_of_edges(),
        'density': nx.density(G),
        'components': list(nx.connected_components(G)),
        'centrality': {},
        'key_players': []
    }
    
    if G.number_of_nodes() > 0:
        # Centralit√©
        analysis['centrality'] = {
            'degree': nx.degree_centrality(G),
            'betweenness': nx.betweenness_centrality(G) if G.number_of_nodes() > 2 else {},
            'closeness': nx.closeness_centrality(G) if nx.is_connected(G) else {}
        }
        
        # Identifier les acteurs cl√©s (top 5 par centralit√©)
        degree_sorted = sorted(analysis['centrality']['degree'].items(), key=lambda x: x[1], reverse=True)
        analysis['key_players'] = [node for node, _ in degree_sorted[:5]]
    
    return analysis

def create_network_visualization(G: nx.Graph, analysis: dict):
    """Cr√©e la visualisation du r√©seau avec Plotly"""
    
    # Positions des n≈ìuds
    pos = nx.spring_layout(G, k=1, iterations=50)
    
    # Cr√©er les traces pour les ar√™tes
    edge_trace = []
    for edge in G.edges():
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        
        edge_trace.append(go.Scatter(
            x=[x0, x1, None],
            y=[y0, y1, None],
            mode='lines',
            line=dict(width=0.5, color='#888'),
            hoverinfo='none'
        ))
    
    # Cr√©er la trace pour les n≈ìuds
    node_x = []
    node_y = []
    node_text = []
    node_size = []
    node_color = []
    
    for node in G.nodes():
        x, y = pos[node]
        node_x.append(x)
        node_y.append(y)
        
        # Texte du hover
        node_data = G.nodes[node]
        hover_text = f"<b>{node}</b><br>"
        hover_text += f"Type: {node_data.get('type', 'Unknown')}<br>"
        hover_text += f"R√¥le: {node_data.get('role', 'N/A')}<br>"
        hover_text += f"Connexions: {G.degree(node)}"
        node_text.append(hover_text)
        
        # Taille selon le degr√©
        node_size.append(10 + G.degree(node) * 5)
        
        # Couleur selon le type
        if node_data.get('type') == 'person':
            node_color.append('lightblue')
        elif node_data.get('type') == 'company':
            node_color.append('lightgreen')
        else:
            node_color.append('lightgray')
    
    node_trace = go.Scatter(
        x=node_x,
        y=node_y,
        mode='markers+text',
        hoverinfo='text',
        text=[node for node in G.nodes()],
        textposition="top center",
        hovertext=node_text,
        marker=dict(
            showscale=True,
            colorscale='YlGnBu',
            size=node_size,
            color=node_color,
            line_width=2
        )
    )
    
    # Cr√©er la figure
    fig = go.Figure(data=edge_trace + [node_trace])
    
    fig.update_layout(
        title="Cartographie des relations",
        titlefont_size=16,
        showlegend=False,
        hovermode='closest',
        margin=dict(b=20, l=5, r=5, t=40),
        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        height=700
    )
    
    return fig

def process_comparison_request(query: str, analysis: dict):
    """Traite une demande de comparaison"""
    
    # 1. Collecter les documents √† comparer
    documents = collect_comparison_documents(analysis)
    
    if not documents or len(documents) < 2:
        st.warning("‚ö†Ô∏è Au moins 2 documents n√©cessaires pour la comparaison")
        return
    
    # 2. Type de comparaison
    comparison_type = analysis['details'].get('comparison_type', 'general')
    
    # 3. Extraire et comparer
    if comparison_type == 'auditions':
        comparison_result = compare_auditions(documents)
    else:
        comparison_result = compare_documents_general(documents)
    
    # 4. Cr√©er les visualisations
    visualizations = create_comparison_visualizations(comparison_result)
    
    # 5. Stocker les r√©sultats
    st.session_state.comparison_result = {
        'comparison': comparison_result,
        'visualizations': visualizations,
        'type': comparison_type,
        'document_count': len(documents),
        'timestamp': datetime.now()
    }

def compare_auditions(documents: list) -> dict:
    """Compare sp√©cifiquement des auditions"""
    
    llm_manager = MultiLLMManager()
    if not llm_manager.clients:
        return {}
    
    # Prompt sp√©cialis√© pour comparaison d'auditions
    prompt = """Compare ces auditions/d√©clarations en d√©tail.

Identifie pour chaque point abord√© :
1. CONVERGENCES : Points sur lesquels toutes les versions concordent
2. DIVERGENCES : Points de d√©saccord ou contradictions
3. √âVOLUTIONS : Changements de version d'une m√™me personne
4. OMISSIONS : Points mentionn√©s par certains mais pas d'autres
5. D√âTAILS SUSPECTS : Incoh√©rences, impossibilit√©s factuelles

Pour chaque point, cite pr√©cis√©ment les d√©clarations.

Format structur√© :
### CONVERGENCES
- Point 1 : [description]
  - Personne A : "citation exacte"
  - Personne B : "citation exacte"

### DIVERGENCES
- Point 1 : [description de la divergence]
  - Version A : "citation"
  - Version B : "citation contradictoire"

### √âVOLUTIONS
- Personne X :
  - Premi√®re version : "citation"
  - Version ult√©rieure : "citation modifi√©e"

### ANALYSE
- Fiabilit√© globale
- Points n√©cessitant v√©rification
- Recommandations pour l'enqu√™te"""
    
    # Pr√©parer le contexte
    context = "\n\n".join([
        f"=== AUDITION/DOCUMENT: {doc['title']} ===\n{doc['content'][:3000]}"
        for doc in documents[:10]  # Limiter
    ])
    
    full_prompt = f"{prompt}\n\nDocuments √† comparer :\n{context}"
    
    try:
        provider = list(llm_manager.clients.keys())[0]
        response = llm_manager.query_single_llm(
            provider,
            full_prompt,
            "Tu es un expert en analyse comparative de t√©moignages et proc√©dures judiciaires."
        )
        
        if response['success']:
            # Parser la r√©ponse structur√©e
            comparison = parse_comparison_response(response['response'])
            
            # Ajouter une analyse quantitative
            comparison['statistics'] = calculate_comparison_statistics(comparison)
            
            return comparison
            
    except Exception as e:
        st.error(f"Erreur comparaison : {e}")
        return {}

def parse_comparison_response(response: str) -> dict:
    """Parse la r√©ponse de comparaison structur√©e"""
    comparison = {
        'convergences': [],
        'divergences': [],
        'evolutions': [],
        'omissions': [],
        'suspicious_details': [],
        'analysis': ''
    }
    
    # Extraire chaque section
    sections = {
        'convergences': r'###?\s*CONVERGENCES?\s*\n(.*?)(?=###|$)',
        'divergences': r'###?\s*DIVERGENCES?\s*\n(.*?)(?=###|$)',
        'evolutions': r'###?\s*[√âE]VOLUTIONS?\s*\n(.*?)(?=###|$)',
        'analysis': r'###?\s*ANALYSE\s*\n(.*?)$'
    }
    
    for key, pattern in sections.items():
        match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)
        if match:
            content = match.group(1)
            
            if key == 'analysis':
                comparison[key] = content.strip()
            else:
                # Extraire les points
                points = re.findall(r'-\s*([^:]+):\s*([^\n]+(?:\n\s+[^\n]+)*)', content)
                comparison[key] = [{'point': p[0].strip(), 'details': p[1].strip()} for p in points]
    
    return comparison

def calculate_comparison_statistics(comparison: dict) -> dict:
    """Calcule des statistiques sur la comparaison"""
    return {
        'convergence_rate': len(comparison['convergences']) / max(
            len(comparison['convergences']) + len(comparison['divergences']), 1
        ),
        'total_points': len(comparison['convergences']) + len(comparison['divergences']),
        'evolution_count': len(comparison['evolutions']),
        'reliability_score': calculate_reliability_score(comparison)
    }

def calculate_reliability_score(comparison: dict) -> float:
    """Calcule un score de fiabilit√© bas√© sur la comparaison"""
    score = 0.5  # Base
    
    # Plus de convergences = plus fiable
    convergence_rate = len(comparison['convergences']) / max(
        len(comparison['convergences']) + len(comparison['divergences']), 1
    )
    score += convergence_rate * 0.3
    
    # √âvolutions = moins fiable
    if len(comparison['evolutions']) > 0:
        score -= min(len(comparison['evolutions']) * 0.1, 0.3)
    
    # D√©tails suspects = moins fiable
    if len(comparison.get('suspicious_details', [])) > 0:
        score -= min(len(comparison['suspicious_details']) * 0.05, 0.2)
    
    return max(0, min(1, score))

def create_comparison_visualizations(comparison_result: dict):
    """Cr√©e les visualisations pour la comparaison"""
    visualizations = {}
    
    # 1. Graphique en radar des convergences/divergences
    if 'statistics' in comparison_result:
        stats = comparison_result['statistics']
        
        categories = ['Convergences', 'Divergences', '√âvolutions', 'Fiabilit√©']
        values = [
            len(comparison_result.get('convergences', [])),
            len(comparison_result.get('divergences', [])),
            len(comparison_result.get('evolutions', [])),
            stats.get('reliability_score', 0) * 10  # Mettre √† l'√©chelle
        ]
        
        fig_radar = go.Figure(data=go.Scatterpolar(
            r=values,
            theta=categories,
            fill='toself',
            name='Analyse comparative'
        ))
        
        fig_radar.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, max(values) * 1.2]
                )
            ),
            showlegend=False,
            title="Vue d'ensemble de la comparaison"
        )
        
        visualizations['radar'] = fig_radar
    
    # 2. Timeline des √©volutions si applicable
    if comparison_result.get('evolutions'):
        # Cr√©er une timeline des changements de version
        # (Simplifi√©e ici, pourrait √™tre plus sophistiqu√©e)
        visualizations['evolution_timeline'] = create_evolution_timeline(comparison_result['evolutions'])
    
    # 3. Heatmap des points de comparaison
    visualizations['heatmap'] = create_comparison_heatmap(comparison_result)
    
    return visualizations

def create_evolution_timeline(evolutions: list):
    """Cr√©e une timeline des √©volutions de d√©clarations"""
    # Impl√©mentation simplifi√©e
    fig = go.Figure()
    
    for i, evolution in enumerate(evolutions):
        fig.add_trace(go.Scatter(
            x=[0, 1],
            y=[i, i],
            mode='lines+markers+text',
            name=f"√âvolution {i+1}",
            text=['Version initiale', 'Version modifi√©e'],
            textposition='top center'
        ))
    
    fig.update_layout(
        title="√âvolutions des d√©clarations",
        xaxis_title="Versions",
        yaxis_title="Points modifi√©s",
        height=400
    )
    
    return fig

def create_comparison_heatmap(comparison_result: dict):
    """Cr√©e une heatmap des points de comparaison"""
    # Construire une matrice simple
    points = []
    categories = []
    
    for conv in comparison_result.get('convergences', []):
        points.append(conv['point'])
        categories.append('Convergence')
    
    for div in comparison_result.get('divergences', []):
        points.append(div['point'])
        categories.append('Divergence')
    
    for evo in comparison_result.get('evolutions', []):
        points.append(evo.get('point', '√âvolution'))
        categories.append('√âvolution')
    
    # Cr√©er une matrice binaire simple
    matrix = []
    for i, cat in enumerate(categories):
        row = [0] * len(set(categories))
        row[list(set(categories)).index(cat)] = 1
        matrix.append(row)
    
    fig = go.Figure(data=go.Heatmap(
        z=matrix,
        x=list(set(categories)),
        y=points,
        colorscale='RdYlGn',
        showscale=False
    ))
    
    fig.update_layout(
        title="R√©partition des points de comparaison",
        xaxis_title="Type",
        yaxis_title="Points analys√©s",
        height=max(400, len(points) * 30)
    )
    
    return fig

# Modifier show_unified_results_tab pour afficher ces nouveaux r√©sultats
def show_unified_results_tab():
    """Affiche les r√©sultats unifi√©s (VERSION √âTENDUE)"""
    
    # R√©sultats de chronologie
    if 'timeline_result' in st.session_state:
        show_timeline_results()
    
    # R√©sultats de cartographie
    elif 'mapping_result' in st.session_state:
        show_mapping_results()
    
    # R√©sultats de comparaison
    elif 'comparison_result' in st.session_state:
        show_comparison_results()
    
    # R√©sultats existants (r√©daction, analyse, recherche)
    elif 'redaction_result' in st.session_state:
        show_redaction_results()
    
    elif 'ai_analysis_results' in st.session_state and st.session_state.ai_analysis_results:
        show_ai_analysis_results()
    
    elif 'search_results' in st.session_state and st.session_state.search_results:
        show_search_results_unified()
    
    else:
        st.info("üîç Utilisez la barre de recherche universelle pour commencer")
        show_extended_examples()

def show_timeline_results():
    """Affiche les r√©sultats de chronologie"""
    result = st.session_state.timeline_result
    
    st.markdown("### ‚è±Ô∏è Chronologie")
    
    # M√©tadonn√©es
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("√âv√©nements", len(result['events']))
    
    with col2:
        timeline_types = {
            'facts': 'üìÖ Faits',
            'procedure': '‚öñÔ∏è Proc√©dure',
            'general': 'üìä G√©n√©rale'
        }
        st.metric("Type", timeline_types.get(result['type'], result['type']))
    
    with col3:
        st.metric("Documents analys√©s", result['document_count'])
    
    # Visualisation principale
    if result['visualization']:
        st.plotly_chart(result['visualization'], use_container_width=True)
    
    # Liste d√©taill√©e des √©v√©nements
    with st.expander("üìã D√©tail des √©v√©nements", expanded=False):
        for event in result['events']:
            col1, col2 = st.columns([1, 3])
            
            with col1:
                st.write(f"**{event.get('date', 'N/A')}**")
            
            with col2:
                st.write(event.get('description', ''))
                if 'actors' in event:
                    st.caption(f"Acteurs : {', '.join(event['actors'])}")
                if 'source' in event:
                    st.caption(f"Source : {event['source']}")
    
    # Actions
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üíæ Exporter chronologie", key="export_timeline"):
            export_timeline(result)
    
    with col2:
        if st.button("üìä Analyser p√©riodes", key="analyze_periods"):
            analyze_timeline_periods(result)
    
    with col3:
        if st.button("üîç Zoomer p√©riode", key="zoom_period"):
            zoom_timeline_period(result)

def show_mapping_results():
    """Affiche les r√©sultats de cartographie"""
    result = st.session_state.mapping_result
    
    st.markdown("### üó∫Ô∏è Cartographie des relations")
    
    # M√©tadonn√©es
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Entit√©s", result['analysis']['node_count'])
    
    with col2:
        st.metric("Relations", result['analysis']['edge_count'])
    
    with col3:
        st.metric("Densit√©", f"{result['analysis']['density']:.2f}")
    
    with col4:
        entity_types = {
            'persons': 'üë§ Personnes',
            'companies': 'üè¢ Soci√©t√©s',
            'all': 'üîç Toutes'
        }
        st.metric("Type", entity_types.get(result['type'], result['type']))
    
    # Visualisation du r√©seau
    if result['visualization']:
        st.plotly_chart(result['visualization'], use_container_width=True)
    
    # Acteurs cl√©s
    if result['analysis']['key_players']:
        st.markdown("### üéØ Acteurs cl√©s")
        
        cols = st.columns(len(result['analysis']['key_players'][:5]))
        for i, (col, player) in enumerate(zip(cols, result['analysis']['key_players'][:5])):
            with col:
                st.metric(
                    f"#{i+1}",
                    player,
                    f"Centralit√©: {result['analysis']['centrality']['degree'].get(player, 0):.2f}"
                )
    
    # Analyse d√©taill√©e
    with st.expander("üìä Analyse du r√©seau", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**Composantes connexes**")
            for i, component in enumerate(result['analysis']['components']):
                st.write(f"Groupe {i+1}: {len(component)} entit√©s")
        
        with col2:
            st.markdown("**M√©triques de centralit√©**")
            top_central = sorted(
                result['analysis']['centrality']['degree'].items(),
                key=lambda x: x[1],
                reverse=True
            )[:5]
            
            for entity, centrality in top_central:
                st.write(f"‚Ä¢ {entity}: {centrality:.3f}")
    
    # Actions
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üíæ Exporter r√©seau", key="export_network"):
            export_network(result)
    
    with col2:
        if st.button("üîç Analyser clusters", key="analyze_clusters"):
            analyze_network_clusters(result)
    
    with col3:
        if st.button("üìä Matrice relations", key="relation_matrix"):
            show_relation_matrix(result)

def show_comparison_results():
    """Affiche les r√©sultats de comparaison"""
    result = st.session_state.comparison_result
    
    st.markdown("### üîÑ Comparaison de documents")
    
    # M√©tadonn√©es
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Documents", result['document_count'])
    
    with col2:
        comp_types = {
            'auditions': 'üé§ Auditions',
            'versions': 'üìÑ Versions',
            'general': 'üìä G√©n√©ral'
        }
        st.metric("Type", comp_types.get(result['type'], result['type']))
    
    with col3:
        if 'statistics' in result['comparison']:
            st.metric(
                "Convergence",
                f"{result['comparison']['statistics']['convergence_rate']:.0%}"
            )
    
    with col4:
        if 'statistics' in result['comparison']:
            st.metric(
                "Fiabilit√©",
                f"{result['comparison']['statistics']['reliability_score']:.0%}"
            )
    
    # Visualisations
    if 'visualizations' in result:
        # Vue radar
        if 'radar' in result['visualizations']:
            st.plotly_chart(result['visualizations']['radar'], use_container_width=True)
        
        # Tabs pour les d√©tails
        tabs = st.tabs(['‚úÖ Convergences', '‚ùå Divergences', 'üîÑ √âvolutions', 'üìä Analyse'])
        
        with tabs[0]:
            show_convergences(result['comparison'])
        
        with tabs[1]:
            show_divergences(result['comparison'])
        
        with tabs[2]:
            show_evolutions(result['comparison'])
        
        with tabs[3]:
            show_comparison_analysis(result['comparison'])
    
    # Actions
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üíæ Exporter comparaison", key="export_comparison"):
            export_comparison(result)
    
    with col2:
        if st.button("üìù G√©n√©rer rapport", key="generate_comparison_report"):
            generate_comparison_report(result)
    
    with col3:
        if st.button("üîç Approfondir", key="deepen_comparison"):
            deepen_comparison_analysis(result)

def show_convergences(comparison: dict):
    """Affiche les points de convergence"""
    convergences = comparison.get('convergences', [])
    
    if not convergences:
        st.info("Aucune convergence identifi√©e")
        return
    
    for i, conv in enumerate(convergences):
        with st.container():
            st.markdown(f"**{i+1}. {conv['point']}**")
            st.write(conv['details'])

def show_divergences(comparison: dict):
    """Affiche les points de divergence"""
    divergences = comparison.get('divergences', [])
    
    if not divergences:
        st.info("Aucune divergence identifi√©e")
        return
    
    for i, div in enumerate(divergences):
        with st.container():
            st.markdown(f"**{i+1}. {div['point']}**")
            st.write(div['details'])
            
            # Alerte si divergence importante
            if 'critique' in div.get('details', '').lower():
                st.warning("‚ö†Ô∏è Divergence critique d√©tect√©e")

def show_evolutions(comparison: dict):
    """Affiche les √©volutions de d√©clarations"""
    evolutions = comparison.get('evolutions', [])
    
    if not evolutions:
        st.info("Aucune √©volution identifi√©e")
        return
    
    for i, evo in enumerate(evolutions):
        with st.container():
            st.markdown(f"**{i+1}. {evo.get('point', '√âvolution')}**")
            st.write(evo.get('details', ''))

def show_comparison_analysis(comparison: dict):
    """Affiche l'analyse globale de la comparaison"""
    if 'analysis' in comparison:
        st.markdown(comparison['analysis'])
    
    if 'statistics' in comparison:
        st.markdown("### üìä Statistiques")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.metric("Points analys√©s", comparison['statistics']['total_points'])
            st.metric("√âvolutions d√©tect√©es", comparison['statistics']['evolution_count'])
        
        with col2:
            reliability = comparison['statistics']['reliability_score']
            color = "üü¢" if reliability > 0.7 else "üü°" if reliability > 0.4 else "üî¥"
            st.metric("Score de fiabilit√©", f"{color} {reliability:.0%}")

def show_extended_examples():
    """Affiche des exemples √©tendus incluant les nouvelles fonctionnalit√©s"""
    
    st.markdown("### üí° Exemples d'utilisation avanc√©e")
    
    advanced_examples = [
        {
            'category': '‚è±Ô∏è Chronologies',
            'examples': [
                {
                    'title': 'Chronologie des faits',
                    'query': 'chronologie des faits @affaire_martin',
                    'desc': 'G√©n√®re une timeline de tous les √©v√©nements'
                },
                {
                    'title': 'Chronologie proc√©durale',
                    'query': 'chronologie de la proc√©dure @dossier_xyz',
                    'desc': 'Timeline des actes de proc√©dure uniquement'
                }
            ]
        },
        {
            'category': 'üó∫Ô∏è Cartographies',
            'examples': [
                {
                    'title': 'R√©seau de personnes',
                    'query': 'cartographie des personnes @affaire_corruption',
                    'desc': 'Visualise les liens entre individus'
                },
                {
                    'title': 'Relations soci√©t√©s',
                    'query': 'mapping des soci√©t√©s et leurs liens @groupe_abc',
                    'desc': 'Carte des relations capitalistiques'
                }
            ]
        },
        {
            'category': 'üîÑ Comparaisons',
            'examples': [
                {
                    'title': 'Comparer auditions',
                    'query': 'comparer les auditions @affaire_martin',
                    'desc': 'Analyse convergences et contradictions'
                },
                {
                    'title': '√âvolution d√©clarations',
                    'query': 'comparer √©volution des versions @t√©moin_principal',
                    'desc': 'D√©tecte les changements de version'
                }
            ]
        }
    ]
    
    for category in advanced_examples:
        st.markdown(f"**{category['category']}**")
        
        cols = st.columns(len(category['examples']))
        for col, example in zip(cols, category['examples']):
            with col:
                with st.container():
                    st.markdown(f"**{example['title']}**")
                    st.caption(example['desc'])
                    st.code(example['query'])
                    
                    if st.button("Utiliser", key=f"use_{example['query']}"):
                        st.session_state.universal_query = example['query']
                        st.rerun()

# Fonctions d'export et d'actions suppl√©mentaires
def export_timeline(result: dict):
    """Exporte la chronologie"""
    # Format CSV
    events_df = pd.DataFrame(result['events'])
    csv = events_df.to_csv(index=False)
    
    st.download_button(
        "üíæ T√©l√©charger CSV",
        csv,
        f"chronologie_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        "text/csv",
        key="download_timeline_csv"
    )

def export_network(result: dict):
    """Exporte le r√©seau"""
    # Format GraphML
    import io
    buffer = io.BytesIO()
    nx.write_graphml(result['graph'], buffer)
    
    st.download_button(
        "üíæ T√©l√©charger GraphML",
        buffer.getvalue(),
        f"reseau_{datetime.now().strftime('%Y%m%d_%H%M%S')}.graphml",
        "application/xml",
        key="download_network_graphml"
    )

def export_comparison(result: dict):
    """Exporte la comparaison"""
    # Format structur√©
    comparison_text = format_comparison_for_export(result['comparison'])
    
    st.download_button(
        "üíæ T√©l√©charger rapport",
        comparison_text,
        f"comparaison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
        "text/plain",
        key="download_comparison_txt"
    )

def format_comparison_for_export(comparison: dict) -> str:
    """Formate la comparaison pour l'export"""
    output = "RAPPORT DE COMPARAISON\n" + "=" * 50 + "\n\n"
    
    output += f"Date : {datetime.now().strftime('%d/%m/%Y %H:%M')}\n\n"
    
    if 'statistics' in comparison:
        output += f"Taux de convergence : {comparison['statistics']['convergence_rate']:.0%}\n"
        output += f"Score de fiabilit√© : {comparison['statistics']['reliability_score']:.0%}\n\n"
    
    output += "CONVERGENCES\n" + "-" * 30 + "\n"
    for conv in comparison.get('convergences', []):
        output += f"‚Ä¢ {conv['point']}\n  {conv['details']}\n\n"
    
    output += "DIVERGENCES\n" + "-" * 30 + "\n"
    for div in comparison.get('divergences', []):
        output += f"‚Ä¢ {div['point']}\n  {div['details']}\n\n"
    
    if comparison.get('analysis'):
        output += "ANALYSE\n" + "-" * 30 + "\n"
        output += comparison['analysis']
    
    return output

# Fonctions helper pour les documents
def collect_mapping_documents(analysis: dict) -> list:
    """Collecte les documents pour la cartographie"""
    # R√©utiliser la logique de collect_timeline_documents
    return collect_timeline_documents(analysis)

def collect_comparison_documents(analysis: dict) -> list:
    """Collecte les documents pour la comparaison"""
    documents = []
    
    # Si r√©f√©rence sp√©cifique
    if analysis['reference']:
        ref_results = search_by_reference(f"@{analysis['reference']}")
        
        # Filtrer par type si sp√©cifi√©
        if analysis['details'].get('comparison_type') == 'auditions':
            # Chercher sp√©cifiquement les auditions
            audition_keywords = ['audition', 'interrogatoire', 'd√©claration', 't√©moignage']
            documents = [
                doc for doc in ref_results
                if any(kw in doc.get('title', '').lower() or kw in doc.get('content', '').lower()
                      for kw in audition_keywords)
            ]
        else:
            documents = ref_results
    
    return documents

def compare_documents_general(documents: list) -> dict:
    """Comparaison g√©n√©rale de documents"""
    # R√©utiliser la logique de compare_auditions avec un prompt plus g√©n√©ral
    return compare_auditions(documents)

# Fonctions d'analyse suppl√©mentaires (placeholder)
def analyze_timeline_periods(result: dict):
    """Analyse les p√©riodes de la chronologie"""
    st.info("üîÑ Analyse des p√©riodes √† impl√©menter")

def zoom_timeline_period(result: dict):
    """Zoom sur une p√©riode sp√©cifique"""
    st.info("üîç Zoom temporel √† impl√©menter")

def analyze_network_clusters(result: dict):
    """Analyse les clusters du r√©seau"""
    st.info("üîç Analyse de clusters √† impl√©menter")

def show_relation_matrix(result: dict):
    """Affiche la matrice des relations"""
    st.info("üìä Matrice de relations √† impl√©menter")

def generate_comparison_report(result: dict):
    """G√©n√®re un rapport de comparaison d√©taill√©"""
    st.info("üìù G√©n√©ration de rapport √† impl√©menter")

def deepen_comparison_analysis(result: dict):
    """Approfondit l'analyse comparative"""
    st.info("üî¨ Analyse approfondie √† impl√©menter")