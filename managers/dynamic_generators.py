# managers/dynamic_generators.py
"""G√©n√©rateurs dynamiques pour prompts et templates"""

import json
import re
import logging
import asyncio
from typing import Dict, List, Any

logger = logging.getLogger(__name__)

from managers.multi_llm_manager import MultiLLMManager
from config.app_config import LLMProvider

async def generate_dynamic_search_prompts(search_query: str, context: str = "") -> Dict[str, Dict[str, List[str]]]:
    """G√©n√®re dynamiquement des prompts de recherche bas√©s sur la requ√™te"""
    llm_manager = MultiLLMManager()
    
    # Utiliser Claude Opus 4 et ChatGPT 4o si disponibles
    preferred_providers = []
    if LLMProvider.CLAUDE_OPUS in llm_manager.clients:
        preferred_providers.append(LLMProvider.CLAUDE_OPUS)
    if LLMProvider.CHATGPT_4O in llm_manager.clients:
        preferred_providers.append(LLMProvider.CHATGPT_4O)
    
    if not preferred_providers and llm_manager.clients:
        preferred_providers = [list(llm_manager.clients.keys())[0]]
    
    if not preferred_providers:
        # Retour aux prompts statiques si aucun LLM disponible
        return {
            "üîç Recherches sugg√©r√©es": {
                "G√©n√©rique": [
                    f"{search_query} jurisprudence r√©cente",
                    f"{search_query} √©l√©ments constitutifs",
                    f"{search_query} moyens de d√©fense",
                    f"{search_query} sanctions encourues"
                ]
            }
        }
    
    prompt = f"""En tant qu'expert en droit p√©nal des affaires, g√©n√®re des prompts de recherche juridique pertinents bas√©s sur cette requ√™te : "{search_query}"
{f"Contexte suppl√©mentaire : {context}" if context else ""}
Cr√©e une structure JSON avec des cat√©gories et sous-cat√©gories de prompts de recherche.
Chaque prompt doit √™tre concis (max 80 caract√®res) et cibler un aspect juridique pr√©cis.
Format attendu :
{{
    "üîç √âl√©ments constitutifs": {{
        "√âl√©ment mat√©riel": ["prompt1", "prompt2", ...],
        "√âl√©ment intentionnel": ["prompt1", "prompt2", ...]
    }},
    "‚öñÔ∏è Jurisprudence": {{
        "D√©cisions r√©centes": ["prompt1", "prompt2", ...],
        "Arr√™ts de principe": ["prompt1", "prompt2", ...]
    }},
    "üõ°Ô∏è Moyens de d√©fense": {{
        "Exceptions": ["prompt1", "prompt2", ...],
        "Strat√©gies": ["prompt1", "prompt2", ...]
    }}
}}
G√©n√®re au moins 3 cat√©gories avec 2 sous-cat√©gories chacune, et 4 prompts par sous-cat√©gorie."""
    
    system_prompt = """Tu es un avocat sp√©cialis√© en droit p√©nal des affaires avec 20 ans d'exp√©rience.
Tu ma√Ætrises parfaitement la recherche juridique et sais formuler des requ√™tes pr√©cises pour trouver
la jurisprudence, la doctrine et les textes pertinents. Tes prompts sont toujours en fran√ßais,
techniquement pr√©cis et adapt√©s au contexte du droit p√©nal √©conomique fran√ßais."""
    
    try:
        response = await llm_manager.query_single_llm(
            preferred_providers[0],
            prompt,
            system_prompt
        )
        
        if response['success']:
            # Extraire le JSON de la r√©ponse
            json_match = re.search(r'\{[\s\S]*\}', response['response'])
            if json_match:
                try:
                    return json.loads(json_match.group(0))
                except json.JSONDecodeError:
                    pass
        
    except Exception as e:
        logger.error(f"Erreur g√©n√©ration prompts dynamiques: {e}")
    
    # Fallback
    return {
        "üîç Recherches sugg√©r√©es": {
            "G√©n√©rique": [
                f"{search_query} jurisprudence",
                f"{search_query} √©l√©ments constitutifs",
                f"{search_query} d√©fense",
                f"{search_query} sanctions"
            ]
        }
    }

async def generate_dynamic_templates(type_acte: str, context: Dict[str, Any] = None) -> Dict[str, str]:
    """G√©n√®re dynamiquement des mod√®les d'actes juridiques"""
    llm_manager = MultiLLMManager()
    
    # Utiliser Claude Opus 4 et ChatGPT 4o si disponibles
    preferred_providers = []
    if LLMProvider.CLAUDE_OPUS in llm_manager.clients:
        preferred_providers.append(LLMProvider.CLAUDE_OPUS)
    if LLMProvider.CHATGPT_4O in llm_manager.clients:
        preferred_providers.append(LLMProvider.CHATGPT_4O)
    
    if not preferred_providers and llm_manager.clients:
        preferred_providers = [list(llm_manager.clients.keys())[0]]
    
    if not preferred_providers:
        return {}
    
    context_str = ""
    if context:
        context_str = f"""
Contexte sp√©cifique :
- Client : {context.get('client', 'Non sp√©cifi√©')}
- Infraction : {context.get('infraction', 'Non sp√©cifi√©e')}
- Juridiction : {context.get('juridiction', 'Non sp√©cifi√©e')}
"""
    
    prompt = f"""G√©n√®re 3 mod√®les d'actes juridiques pour : "{type_acte}"
{context_str}
Pour chaque mod√®le, fournis :
1. Un titre descriptif avec emoji (ex: "üì® Demande d'audition libre")
2. Le contenu complet du mod√®le avec les balises [CHAMP] pour les √©l√©ments √† personnaliser
Utilise un style juridique professionnel, formel et conforme aux usages du barreau fran√ßais.
Les mod√®les doivent √™tre imm√©diatement utilisables par un avocat.
Format de r√©ponse attendu (JSON) :
{{
    "üìÑ Mod√®le standard de {type_acte}": "Contenu du mod√®le...",
    "‚öñÔ∏è Mod√®le approfondi de {type_acte}": "Contenu du mod√®le...",
    "üîç Mod√®le d√©taill√© de {type_acte}": "Contenu du mod√®le..."
}}"""
    
    system_prompt = """Tu es un avocat au barreau de Paris, sp√©cialis√© en droit p√©nal des affaires.
Tu r√©diges des actes juridiques depuis 20 ans et ma√Ætrises parfaitement les formules consacr√©es,
la structure des actes et les mentions obligatoires. Tes mod√®les sont toujours conformes
aux exigences proc√©durales et aux usages de la profession."""
    
    try:
        # Interroger les LLMs
        responses = await llm_manager.query_multiple_llms(
            preferred_providers,
            prompt,
            system_prompt
        )
        
        # Fusionner les r√©ponses si plusieurs LLMs
        if len(responses) > 1:
            fusion_prompt = f"""Voici plusieurs propositions de mod√®les pour "{type_acte}".
Fusionne-les intelligemment pour cr√©er les 3 meilleurs mod√®les en gardant le meilleur de chaque proposition.
{chr(10).join([f"Proposition {i+1}: {r['response']}" for i, r in enumerate(responses) if r['success']])}
Retourne un JSON avec 3 mod√®les fusionn√©s."""
            
            fusion_response = await llm_manager.query_single_llm(
                preferred_providers[0],
                fusion_prompt,
                "Tu es un expert en fusion de contenus juridiques."
            )
            
            if fusion_response['success']:
                response_text = fusion_response['response']
            else:
                response_text = responses[0]['response'] if responses[0]['success'] else ""
        else:
            response_text = responses[0]['response'] if responses and responses[0]['success'] else ""
        
        # Extraire le JSON
        if response_text:
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                try:
                    return json.loads(json_match.group(0))
                except json.JSONDecodeError:
                    pass
        
    except Exception as e:
        logger.error(f"Erreur g√©n√©ration mod√®les dynamiques: {e}")
    
    # Fallback avec un mod√®le basique
    return {
        f"üìÑ Mod√®le standard de {type_acte}": f"""[EN-T√äTE AVOCAT]
√Ä l'attention de [DESTINATAIRE]
Objet : {type_acte}
R√©f√©rence : [R√âF√âRENCE]
[FORMULE D'APPEL],
J'ai l'honneur de [OBJET DE LA DEMANDE].
[D√âVELOPPEMENT]
[CONCLUSION]
Je vous prie d'agr√©er, [FORMULE DE POLITESSE].
[SIGNATURE]"""
    }